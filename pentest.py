# Orchestrates per-resource pentest runs (start → poll → download → outcome)
# - start error classification
# - polling with graceful 401/403 handling
# - extended GraphQL polling that aborts on permission errors
# - rolling parallel runner with start-retry queue
# - system prompt configuration before pentest execution
#
# Depends on: config.py, api.py, auth.py

from __future__ import annotations

import time
import random
import threading
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError
from typing import Any, Dict, List, Optional

import requests

import config
import api
import auth

# ---------------------------
# Start error classification
# ---------------------------

def is_retryable_start_error(e: Exception) -> bool:
    """Return True if starting the pentest should be retried."""
    if isinstance(e, requests.HTTPError):
        status_code = e.response.status_code
        if 500 <= status_code < 600:
            return True
        if status_code == 429:
            return True
        if status_code in (401, 403):
            return False
        if status_code == 400:
            try:
                txt = (e.response.text or "").lower()
                non_retryable_400 = [
                    "validation error",
                    "invalid api key",
                    "exceeded your current quota",
                    "unable to load pentest connection details",
                    "dsl script",
                ]
                if any(k in txt for k in non_retryable_400):
                    return False
                return True
            except Exception:
                return True
    # network or unknown => retry
    return True

# ---------------------------
# Per-resource pentest runner
# ---------------------------

def run_pentest_for_resource(
    resource_id: str, 
    resource_name: str, 
    template_id: str,
    resource_type: Optional[str] = None
) -> Dict[str, Any]:
    thread_name = threading.current_thread().name
    print(f"\n{'='*60}")
    print(f"[Thread: {thread_name}] Starting pentest for: {resource_name}")
    print(f"[Thread: {thread_name}] Resource ID: {resource_id}")
    if resource_type:
        print(f"[Thread: {thread_name}] Resource Type: {resource_type}")
    print(f"{'='*60}")

    # Get thread-local JWT token
    jwt_token = auth.get_thread_jwt_token()

    # Step 0: Configure system prompt on resource if enabled
    system_prompt_configured = False
    if config.PENTEST_SYSTEM_PROMPT_ENABLED and config.PENTEST_SYSTEM_PROMPT_TEXT:
        print(f"[Thread: {thread_name}][i] Configuring system prompt on resource...")
        try:
            api.configure_llm_endpoint_system_prompt(
                jwt_token=jwt_token,
                resource_instance_id=resource_id,
                system_prompt=config.PENTEST_SYSTEM_PROMPT_TEXT,
            )
            print(f"[Thread: {thread_name}][✓] System prompt configured successfully")
            system_prompt_configured = True
        except Exception as e:
            print(f"[Thread: {thread_name}][!] Warning: Failed to configure system prompt: {e}")
            # Continue anyway - the start-pentest call will use whatever is already configured

    # Step 1: Check if we have a model mapping for this resource type
    pentest_connection_details = {}
    
    if resource_type and resource_type in config.PENTEST_MODEL_MAPPING:
        preferred_model = config.PENTEST_MODEL_MAPPING[resource_type]
        print(f"[Thread: {thread_name}][i] Model mapping found for {resource_type}: {preferred_model}")
        
        # Fetch available models for this specific resource to validate
        available_models = api.get_llm_pentest_models(jwt_token, resource_id)
        
        if available_models and preferred_model in available_models:
            pentest_connection_details["model"] = preferred_model
            print(f"[Thread: {thread_name}][✓] Using mapped model: {preferred_model}")
        elif available_models:
            print(f"[Thread: {thread_name}][!] Mapped model '{preferred_model}' not available")
            print(f"[Thread: {thread_name}][i] Available: {', '.join(available_models[:5])}{'...' if len(available_models) > 5 else ''}")
            print(f"[Thread: {thread_name}][i] Using endpoint default")
        else:
            print(f"[Thread: {thread_name}][!] Could not fetch available models")
            print(f"[Thread: {thread_name}][i] Using endpoint default")
    else:
        if resource_type:
            print(f"[Thread: {thread_name}][i] No model mapping for {resource_type}; using endpoint default")
        else:
            print(f"[Thread: {thread_name}][i] Resource type unknown; using endpoint default")
    
    # Build request payload
    data = {
        "resource_instance_id": resource_id,
        "llm_pentest_scan_template_id": template_id,
        "description": f"CI/CD Pentest for {resource_name}",
        "pentest_connection_details": pentest_connection_details,
        "system_prompt_enabled": config.PENTEST_SYSTEM_PROMPT_ENABLED,
        "apply_guardrails": config.PENTEST_APPLY_GUARDRAILS,
    }
    
    print(f"[Thread: {thread_name}][i] System prompt enabled: {config.PENTEST_SYSTEM_PROMPT_ENABLED}")
    print(f"[Thread: {thread_name}][i] Apply guardrails: {config.PENTEST_APPLY_GUARDRAILS}")

    # Step 2: Start Pentest
    print(f"\n[Thread: {thread_name}][3] Starting pentest for {resource_name}...")
    start_api = f"/v2/llm-pentest/customer/{config.CUSTOMER_ID}/start-pentest"

    try:
        resp = api.make_api_request(start_api, token=jwt_token, method="POST", data=data)
        resp_json = resp.json()
        job_id = resp_json["job_id"]
        scan_execution_id = resp_json["llm_pentest_scan_execution_id"]
        print(f"[Thread: {thread_name}][+] Pentest Execution ID: {scan_execution_id}")
        print(f"[Thread: {thread_name}][+] Pentest Job ID: {job_id}")
    except Exception as e:
        print(f"[Thread: {thread_name}][-] Failed to start pentest for {resource_name}: {e}")
        # Cleanup system prompt if we configured it
        if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
            _cleanup_system_prompt(jwt_token, resource_id, thread_name)
        
        if is_retryable_start_error(e):
            return {"resource_id": resource_id, "resource_name": resource_name, "status": "START_FAILED_RETRYABLE", "error": str(e)}
        else:
            return {"resource_id": resource_id, "resource_name": resource_name, "status": "START_FAILED", "error": str(e)}
        
    # Step 3: Poll for Pentest Results
    print(f"\n[Thread: {thread_name}][4] Checking pentest execution status for {resource_name}...")
    job_status_api = f"/v2/llm-pentest/job-status/{job_id}"

    # Polling controls
    POLL_INTERVAL_SECS = config.POLL_INTERVAL_SECS
    POLL_TIMEOUT_SECS = config.POLL_TIMEOUT_SECS
    BACKOFF_BASE_SECS = config.POLL_BACKOFF_BASE_SECS
    BACKOFF_MAX_SECS = config.POLL_BACKOFF_MAX_SECS
    NOT_FOUND_GRACE_POLLS = config.POLL_NOT_FOUND_GRACE
    POLL_STATUS_LOG_EVERY = config.POLL_STATUS_LOG_EVERY
    POLL_TIMEOUT_ACTION = config.POLL_TIMEOUT_ACTION  # "fail", "continue" or "partial"

    start_ts = time.time()
    status: Optional[str] = None
    last_logged_status: Optional[str] = None
    poll_counter = 0
    retry_streak = 0
    not_found_streak = 0

    while True:
        elapsed = time.time() - start_ts
        if elapsed >= POLL_TIMEOUT_SECS:
            timeout_msg = f"Polling timed out after {int(POLL_TIMEOUT_SECS)}s for {resource_name}."

            # If last known status was RUNNING, try extended GraphQL polling
            if status == "RUNNING":
                print(f"[Thread: {thread_name}][!] {timeout_msg} Last status was RUNNING - switching to GraphQL polling...")

                graphql_start = time.time()
                graphql_timeout = config.GRAPHQL_EXTENDED_TIMEOUT_SECS
                graphql_interval = config.GRAPHQL_POLL_INTERVAL_SECS

                print(f"[Thread: {thread_name}][+] Starting extended GraphQL polling (timeout: {graphql_timeout}s, interval: {graphql_interval}s)")

                while (time.time() - graphql_start) < graphql_timeout:
                    try:
                        # Use the full execution query and extract only what we need
                        data = api.query_pentest_execution_full(jwt_token, scan_execution_id) or {}
                        exec_min = (data.get("llmPentestScanExecution") or {}) if isinstance(data, dict) else {}
                        outcome = (exec_min.get("outcomeLevel") or "").strip()

                        if outcome:
                            elapsed_extended = time.time() - graphql_start
                            total_elapsed = time.time() - start_ts
                            print(f"[Thread: {thread_name}][+] Pentest completed during extended polling! (Extended: {elapsed_extended:.1f}s, Total: {total_elapsed:.1f}s)")
                            print(f"[Thread: {thread_name}][+] Outcome: {outcome}")

                            api.download_results_csv(jwt_token, resource_name, resource_id, scan_execution_id)
                            
                            # Cleanup system prompt if configured
                            if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                                _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                            
                            return {
                                "resource_id": resource_id,
                                "resource_name": resource_name,
                                "status": "COMPLETED",
                                "outcome": outcome,
                                "execution_details": exec_min,
                                "scan_execution_id": scan_execution_id,
                                "job_id": job_id,
                                "total_duration": total_elapsed,
                                "message": "Completed via extended GraphQL polling"
                            }

                        print(f"[Thread: {thread_name}]    Extended GraphQL poll: still running (elapsed: {time.time() - graphql_start:.1f}s)")
                        time.sleep(graphql_interval)

                    except requests.HTTPError as ge:
                        sc = getattr(getattr(ge, "response", None), "status_code", None)
                        if sc in (401, 403):
                            # Cleanup system prompt if configured
                            if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                                _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                            
                            return {
                                "resource_id": resource_id,
                                "resource_name": resource_name,
                                "status": "GRAPHQL_PERMISSION_DENIED",
                                "error": "GraphQL llmPentestScanExecution permission denied",
                                "scan_execution_id": scan_execution_id,
                                "job_id": job_id,
                            }
                        print(f"[Thread: {thread_name}][!] GraphQL polling error: {ge}")
                        time.sleep(min(graphql_interval, 60))
                    except Exception as e:
                        print(f"[Thread: {thread_name}][!] GraphQL polling error: {e}")
                        time.sleep(min(graphql_interval, 60))

                print(f"[Thread: {thread_name}][-] Extended GraphQL polling timed out after {graphql_timeout}s")
                
                # Cleanup system prompt if configured
                if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                    _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                
                return {
                    "resource_id": resource_id,
                    "resource_name": resource_name,
                    "status": "EXTENDED_POLL_TIMEOUT",
                    "message": f"Pentest still running after extended polling ({int(POLL_TIMEOUT_SECS + graphql_timeout)}s total)",
                    "scan_execution_id": scan_execution_id,
                    "job_id": job_id,
                }

            if POLL_TIMEOUT_ACTION == "continue":
                print(f"[Thread: {thread_name}][!] {timeout_msg} Continuing without downloading results...")
                
                # Cleanup system prompt if configured
                if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                    _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                
                return {
                    "resource_id": resource_id,
                    "resource_name": resource_name,
                    "status": "POLL_TIMEOUT_CONTINUE",
                    "outcome": "Unknown",
                    "message": f"Timed out after {int(POLL_TIMEOUT_SECS)} seconds, pentest may still be running",
                    "scan_execution_id": scan_execution_id,
                    "job_id": job_id,
                }

            elif POLL_TIMEOUT_ACTION == "partial":
                print(f"[Thread: {thread_name}][!] {timeout_msg} Checking if any results available...")
                try:
                    data = api.query_pentest_execution_full(jwt_token, scan_execution_id) or {}
                    exec_min = (data.get("llmPentestScanExecution") or {}) if isinstance(data, dict) else {}
                    outcome = (exec_min.get("outcomeLevel") or "").strip()

                    if outcome:
                        print(f"[Thread: {thread_name}][+] Found outcome during timeout check: {outcome}")
                        api.download_results_csv(jwt_token, resource_name, resource_id, scan_execution_id)
                        
                        # Cleanup system prompt if configured
                        if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                            _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                        
                        return {
                            "resource_id": resource_id,
                            "resource_name": resource_name,
                            "status": "COMPLETED",
                            "outcome": outcome,
                            "scan_execution_id": scan_execution_id,
                            "job_id": job_id,
                            "message": "Completed during timeout check"
                        }

                except Exception as e:
                    print(f"[Thread: {thread_name}][-] Error checking for partial results: {e}")

            print(f"[Thread: {thread_name}][-] {timeout_msg}")
            
            # Cleanup system prompt if configured
            if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                _cleanup_system_prompt(jwt_token, resource_id, thread_name)
            
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "POLL_TIMEOUT",
                "error": f"Timed out after {int(POLL_TIMEOUT_SECS)} seconds",
                "scan_execution_id": scan_execution_id,
                "job_id": job_id,
            }

        try:
            resp = api.make_api_request(job_status_api, token=jwt_token, method="GET", include_api_key=True)
            resp_json = resp.json()
            status = resp_json.get("status")

            poll_counter += 1
            should_log = False
            if status != last_logged_status:
                should_log = True
                last_logged_status = status
            elif status == "RUNNING" and (poll_counter % POLL_STATUS_LOG_EVERY == 0):
                should_log = True

            if should_log:
                print(f"[Thread: {thread_name}]    Current status for {resource_name}: {status}")

            retry_streak = 0

            if status == "COMPLETED":
                break

            if status == "FAILED":
                print(f"[Thread: {thread_name}][-] Pentest failed for {resource_name}")
                
                # Cleanup system prompt if configured
                if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                    _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                
                return {"resource_id": resource_id, "resource_name": resource_name, "status": "PENTEST_FAILED", "job_id": job_id}

            if status == "NOT_FOUND":
                not_found_streak += 1
                if not_found_streak <= NOT_FOUND_GRACE_POLLS:
                    time.sleep(min(POLL_INTERVAL_SECS, 10))
                    continue
                if should_log is False:
                    print(f"[Thread: {thread_name}][!] job-status returned NOT_FOUND (streak={not_found_streak}); continuing to poll…")
            else:
                not_found_streak = 0

            time.sleep(POLL_INTERVAL_SECS)

        except requests.RequestException as e:
            code = getattr(getattr(e, "response", None), "status_code", None)
            if code in (401, 403):
                # Cleanup system prompt if configured
                if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                    _cleanup_system_prompt(jwt_token, resource_id, thread_name)

                return {
                    "resource_id": resource_id,
                    "resource_name": resource_name,
                    "status": "POLL_PERMISSION_DENIED",
                    "error": f"job-status {code}: permission denied",
                    "job_id": job_id,
                }

            retry_streak += 1
            raw_delay = BACKOFF_BASE_SECS * (2 ** min(retry_streak, 6))
            delay = min(BACKOFF_MAX_SECS, raw_delay) * (0.8 + 0.4 * random.random())
            print(
                f"[Thread: {thread_name}][!] Temporary polling error for {resource_name} "
                f"(attempt {retry_streak}, status={code}). Retrying in {delay:.1f}s. Error: {e}"
            )
            time.sleep(delay)

    print(f"[Thread: {threading.current_thread().name}][+] Pentest completed for {resource_name}. Downloading results...")
    api.download_results_csv(jwt_token, resource_name, resource_id, scan_execution_id)

    print(f"\n[Thread: {threading.current_thread().name}][5] Evaluating pentest outcome for {resource_name}...")
    try:
        data = api.query_pentest_execution_full(jwt_token, scan_execution_id) or {}
        execution = (data.get("llmPentestScanExecution") or {}) if isinstance(data, dict) else {}
        if not execution:
            print(f"[Thread: {threading.current_thread().name}][-] No llmPentestScanExecution in GraphQL response for {resource_name}")
            
            # Cleanup system prompt if configured
            if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                _cleanup_system_prompt(jwt_token, resource_id, thread_name)
            
            return {"resource_id": resource_id, "resource_name": resource_name, "status": "NO_EXECUTION_DATA", "scan_execution_id": scan_execution_id, "job_id": job_id}

        outcome = (execution.get("outcomeLevel") or "").strip()
        print(f"[Thread: {threading.current_thread().name}][+] Outcome Level for {resource_name}: {outcome}")

        # Cleanup system prompt if configured
        if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
            _cleanup_system_prompt(jwt_token, resource_id, thread_name)

        return {
            "resource_id": resource_id,
            "resource_name": resource_name,
            "status": "COMPLETED",
            "outcome": outcome,
            "execution_details": execution,
            "scan_execution_id": scan_execution_id,
            "job_id": job_id,
        }

    except requests.HTTPError as ge:
        sc = getattr(getattr(ge, "response", None), "status_code", None)
        if sc in (401, 403):
            # Cleanup system prompt if configured
            if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                _cleanup_system_prompt(jwt_token, resource_id, thread_name)
            
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "GRAPHQL_PERMISSION_DENIED",
                "error": "GraphQL llmPentestScanExecution permission denied",
                "scan_execution_id": scan_execution_id,
                "job_id": job_id,
            }
        print(f"[Thread: {threading.current_thread().name}][-] Error evaluating outcome for {resource_name}: {ge}")
        
        # Cleanup system prompt if configured
        if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
            _cleanup_system_prompt(jwt_token, resource_id, thread_name)
        
        return {"resource_id": resource_id, "resource_name": resource_name, "status": "EVALUATION_FAILED", "error": str(ge), "scan_execution_id": scan_execution_id, "job_id": job_id}
    except Exception as e:
        print(f"[Thread: {threading.current_thread().name}][-] Error evaluating outcome for {resource_name}: {e}")
        
        # Cleanup system prompt if configured
        if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
            _cleanup_system_prompt(jwt_token, resource_id, thread_name)
        
        return {"resource_id": resource_id, "resource_name": resource_name, "status": "EVALUATION_FAILED", "error": str(e), "scan_execution_id": scan_execution_id, "job_id": job_id}


def _cleanup_system_prompt(jwt_token: str, resource_id: str, thread_name: str) -> None:
    """Helper to cleanup system prompt from resource after pentest (used in multiple exit paths)."""
    try:
        api.cleanup_llm_endpoint_system_prompt(jwt_token, resource_id)
        print(f"[Thread: {thread_name}][i] System prompt cleaned up from resource")
    except Exception as e:
        print(f"[Thread: {thread_name}][!] Warning: Failed to cleanup system prompt: {e}")


# ---------------------------
# Rolling parallel with start-retry queue
# ---------------------------

class RetryableResource:
    def __init__(
        self, 
        resource_id: str, 
        resource_name: str, 
        resource_type: Optional[str] = None,
        attempt: int = 1, 
        last_error: Optional[str] = None
    ):
        self.resource_id = resource_id
        self.resource_name = resource_name
        self.resource_type = resource_type
        self.attempt = attempt
        self.last_error = last_error
        self.next_retry_time = time.time() + config.START_RETRY_DELAY if attempt > 1 else 0

    def can_retry(self) -> bool:
        return self.attempt <= config.MAX_START_RETRIES and time.time() >= self.next_retry_time

    def increment_attempt(self, error: Optional[str] = None):
        self.attempt += 1
        self.last_error = error
        if self.attempt <= config.MAX_START_RETRIES:
            self.next_retry_time = time.time() + config.START_RETRY_DELAY

def run_rolling_parallel_with_retry(
    resource_ids: List[str],
    resource_mapping: Dict[str, str],
    resource_type_mapping: Dict[str, str],  # {resource_id -> resource_type}
    template_id: str,
    max_concurrent: int
) -> List[Dict[str, Any]]:
    all_results: List[Dict[str, Any]] = []

    resource_queue: deque[RetryableResource] = deque([
        RetryableResource(
            rid, 
            resource_mapping[rid], 
            resource_type_mapping.get(rid)
        ) for rid in resource_ids
    ])
    retry_queue: deque[RetryableResource] = deque()

    active_futures: Dict[Any, tuple[RetryableResource, float]] = {}
    completed_count = 0

    # remember when we last issued a *start* API call
    last_start_ts: float = 0.0

    print(f"\n{'='*80}")
    print(f"STARTING ROLLING PARALLEL PENTESTS WITH RETRY")
    print(f"Total resources: {len(resource_ids)}")
    print(f"Max concurrent: {max_concurrent}")
    print(f"Max start retries per resource: {config.MAX_START_RETRIES}")
    print(f"Start retry delay: {config.START_RETRY_DELAY}s")
    print(f"{'='*80}")

    with ThreadPoolExecutor(max_workers=max_concurrent, thread_name_prefix="PentestWorker") as executor:

        def get_next_resource() -> Optional[RetryableResource]:
            if retry_queue:
                ready: list[RetryableResource] = []
                not_ready: deque[RetryableResource] = deque()
                while retry_queue:
                    rr = retry_queue.popleft()
                    if rr.can_retry():
                        ready.append(rr)
                    else:
                        not_ready.append(rr)
                retry_queue.extend(not_ready)
                if ready:
                    next_rr = ready[0]
                    for rr in reversed(ready[1:]):
                        retry_queue.appendleft(rr)
                    return next_rr
            if resource_queue:
                return resource_queue.popleft()
            return None

        def start_next_pentest() -> bool:
            """Start the next available pentest if we have capacity, respecting START_STAGGER_SECS."""
            nonlocal last_start_ts

            if len(active_futures) >= max_concurrent:
                return False

            next_resource = get_next_resource()
            if not next_resource:
                return False

            # ---- stagger starts to avoid backend spikes ----
            if config.START_STAGGER_SECS > 0:
                now = time.time()
                wait = config.START_STAGGER_SECS - (now - last_start_ts)
                if wait > 0:
                    # Short sleep (split large waits so the loop remains responsive)
                    sleep_for = min(wait, 2.0)
                    print(f"[ROLLING] Staggering next start by {sleep_for:.1f}s…")
                    time.sleep(sleep_for)
                    # Re-evaluate once (good enough for whole seconds)
                    now = time.time()
                    wait = config.START_STAGGER_SECS - (now - last_start_ts)
                    if wait > 0:
                        time.sleep(wait)

            retry_info = f" (retry {next_resource.attempt}/{config.MAX_START_RETRIES})" if next_resource.attempt > 1 else ""
            print(f"\n[ROLLING] Starting pentest {len(active_futures) + 1}: {next_resource.resource_name}{retry_info}")
            if next_resource.last_error:
                print(f"          Previous error: {next_resource.last_error}")

            # Mark the moment we kick off a *start* call
            last_start_ts = time.time()

            future = executor.submit(
                run_pentest_for_resource,
                next_resource.resource_id,
                next_resource.resource_name,
                template_id,
                next_resource.resource_type 
            )
            active_futures[future] = (next_resource, time.time())
            return True

        def fill_available_slots() -> int:
            started_count = 0
            while start_next_pentest():
                started_count += 1
            return started_count

        initial_started = fill_available_slots()
        print(f"[ROLLING] Started {initial_started} initial pentests")

        while active_futures or resource_queue or retry_queue:
            if not active_futures:
                min_wait_time = float('inf')
                for retry_resource in list(retry_queue):
                    if retry_resource.can_retry():
                        if fill_available_slots() > 0:
                            break
                    else:
                        wait_time = retry_resource.next_retry_time - time.time()
                        if wait_time > 0:
                            min_wait_time = min(min_wait_time, wait_time)
                if not active_futures:
                    if min_wait_time != float('inf') and min_wait_time > 0:
                        print(f"[ROLLING] Waiting {min_wait_time:.1f}s for next retry opportunity...")
                        time.sleep(min(min_wait_time, 5))
                        continue
                    else:
                        break

            if len(active_futures) < max_concurrent:
                started_now = fill_available_slots()
                if started_now > 0:
                    print(f"[ROLLING] Started {started_now} pentest(s) to fill available slots (proactive)")

            try:
                from concurrent.futures import as_completed, TimeoutError as FuturesTimeoutError
                for future in as_completed(list(active_futures.keys()), timeout=1):
                    retry_resource, start_time = active_futures[future]
                    duration = time.time() - start_time
                    completed_count += 1
                    try:
                        result = future.result()
                        if result.get('status') == 'START_FAILED_RETRYABLE':
                            retry_resource.increment_attempt(result.get('error'))
                            if retry_resource.attempt <= config.MAX_START_RETRIES:
                                retry_queue.append(retry_resource)
                                print(f"\n[ROLLING] Start failed ({completed_count}): {retry_resource.resource_name}")
                                print(f"          Error: {result.get('error')}")
                                print(f"          Queued for retry {retry_resource.attempt}/{config.MAX_START_RETRIES} in {config.START_RETRY_DELAY}s")
                            else:
                                print(f"\n[ROLLING] Start failed permanently ({completed_count}): {retry_resource.resource_name}")
                                print(f"          Exceeded max retries ({config.MAX_START_RETRIES})")
                                result['status'] = 'START_FAILED_MAX_RETRIES'
                                result['final_attempt'] = retry_resource.attempt - 1
                                all_results.append(result)
                        else:
                            all_results.append(result)
                            status = result.get('status', 'Unknown')
                            outcome = result.get('outcome', 'N/A')
                            errorish = {
                                "START_FAILED", "START_FAILED_MAX_RETRIES", "PENTEST_FAILED", "EXCEPTION",
                                "POLL_TIMEOUT", "EXTENDED_POLL_TIMEOUT",
                                "GRAPHQL_PERMISSION_DENIED", "POLL_PERMISSION_DENIED",
                                "NO_EXECUTION_DATA", "EVALUATION_FAILED",
                            }
                            label = "Finished with error" if status in errorish else "Completed"
                            retry_info = f" (after {retry_resource.attempt} attempts)" if retry_resource.attempt > 1 else ""
                            print(f"\n[ROLLING] {label} ({completed_count}): {retry_resource.resource_name}{retry_info}")
                            print(f"          Status: {status}, Outcome: {outcome}, Duration: {duration:.1f}s")
                    except Exception as e:
                        print(f"\n[ROLLING] Exception ({completed_count}): {retry_resource.resource_name} - {e}")
                        all_results.append({
                            "resource_id": retry_resource.resource_id,
                            "resource_name": retry_resource.resource_name,
                            "status": "EXCEPTION",
                            "error": str(e),
                        })
                    del active_futures[future]
                    new_started = fill_available_slots()
                    if new_started > 0:
                        print(f"          Started {new_started} new pentest(s) to fill available slots")
                    break
            except FuturesTimeoutError:
                ready_retries: list[RetryableResource] = []
                remaining_retries: list[RetryableResource] = []
                while retry_queue:
                    retry_resource = retry_queue.popleft()
                    if retry_resource.can_retry():
                        ready_retries.append(retry_resource)
                    elif retry_resource.attempt <= config.MAX_START_RETRIES:
                        remaining_retries.append(retry_resource)
                for rr in reversed(remaining_retries):
                    retry_queue.appendleft(rr)
                started_retries = 0
                for retry_resource in ready_retries:
                    if len(active_futures) < max_concurrent:
                        retry_info = f" (retry {retry_resource.attempt}/{config.MAX_START_RETRIES})"
                        print(f"\n[ROLLING] Starting retry: {retry_resource.resource_name}{retry_info}")
                        if retry_resource.last_error:
                            print(f"          Previous error: {retry_resource.last_error}")
                        future = executor.submit(run_pentest_for_resource, retry_resource.resource_id, retry_resource.resource_name, template_id, retry_resource.resource_type)
                        active_futures[future] = (retry_resource, time.time())
                        started_retries += 1
                    else:
                        retry_queue.appendleft(retry_resource)
                        break
                if started_retries > 0:
                    print(f"          Started {started_retries} retry pentest(s)")
                continue

    print(f"\n[ROLLING] All pentests completed! Total: {len(all_results)} results")

    while retry_queue:
        retry_resource = retry_queue.popleft()
        print(f"[ROLLING] Resource {retry_resource.resource_name} exceeded max retries ({config.MAX_START_RETRIES})")
        all_results.append({
            "resource_id": retry_resource.resource_id,
            "resource_name": retry_resource.resource_name,
            "status": "START_FAILED_MAX_RETRIES",
            "error": retry_resource.last_error,
            "final_attempt": retry_resource.attempt,
        })

    return all_results

