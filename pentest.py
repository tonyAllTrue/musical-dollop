# Orchestrates per-resource pentest runs (start → poll → download → outcome)
# - start error classification
# - polling with graceful 401/403 handling
# - extended GraphQL polling that aborts on permission errors
# - rolling parallel runner with start-retry queue
# - system prompt configuration before pentest execution
# - dataset configuration before pentest execution
#
# Depends on: config.py, api.py, auth.py

from __future__ import annotations

import time
import threading
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError
from typing import Any, Dict, List, Optional

import requests

import config
import api
import auth

# ---------------------------
# Start error classification
# ---------------------------

def is_retryable_start_error(e: Exception) -> bool:
    """Return True if starting the pentest should be retried."""
    if isinstance(e, requests.HTTPError):
        status_code = e.response.status_code
        if 500 <= status_code < 600:
            return True
        if status_code == 429:
            return True
        if status_code in (401, 403):
            return False
        if status_code == 400:
            try:
                txt = (e.response.text or "").lower()
                non_retryable_400 = [
                    "validation error",
                    "invalid api key",
                    "exceeded your current quota",
                    "unable to load pentest connection details",
                    "dsl script",
                ]
                if any(k in txt for k in non_retryable_400):
                    return False
                return True
            except Exception:
                return True
    # network or unknown => retry
    return True

# ---------------------------
# Per-resource pentest runner
# ---------------------------

def run_pentest_for_resource(
    resource_id: str, 
    resource_name: str, 
    template_id: str,
    resource_type: Optional[str] = None,
    project_id: Optional[str] = None,
) -> Dict[str, Any]:
    thread_name = threading.current_thread().name
    print(f"\n{'='*60}")
    print(f"[Thread: {thread_name}] Starting pentest for: {resource_name}")
    print(f"[Thread: {thread_name}] Resource ID: {resource_id}")
    if resource_type:
        print(f"[Thread: {thread_name}] Resource Type: {resource_type}")
    if project_id:
        print(f"[Thread: {thread_name}] Project ID: {project_id}")
    print(f"{'='*60}")

    # Get thread-local JWT token
    jwt_token = auth.get_thread_jwt_token()
    # Step 0: Configure resource additional config (system prompt / dataset / system description) safely.
    system_prompt_configured = False
    dataset_configured = False
    system_description_configured = False
    resolved_dataset_id = None

    desired_system_prompt = None
    if config.PENTEST_SYSTEM_PROMPT_ENABLED and config.PENTEST_SYSTEM_PROMPT_TEXT:
        desired_system_prompt = config.PENTEST_SYSTEM_PROMPT_TEXT

    desired_system_description = None
    if config.PENTEST_RESOURCE_SYSTEM_DESCRIPTION_ENABLED and config.PENTEST_RESOURCE_SYSTEM_DESCRIPTION_TEXT:
        desired_system_description = config.PENTEST_RESOURCE_SYSTEM_DESCRIPTION_TEXT

    desired_dataset_id = None
    if config.PENTEST_DATASET_ENABLED:
        print(f"[Thread: {thread_name}][i] Configuring dataset for capture-replay...")
        # Resolve dataset name to ID if needed
        if config.PENTEST_DATASET_NAME and project_id:
            print(f"[Thread: {thread_name}][i] Resolving dataset name '{config.PENTEST_DATASET_NAME}'...")
            try:
                resolved_dataset_id = api.resolve_dataset_name_to_id(
                    jwt_token=jwt_token,
                    dataset_name=config.PENTEST_DATASET_NAME,
                    project_id=project_id,
                )
                if resolved_dataset_id:
                    print(f"[Thread: {thread_name}][✓] Resolved dataset '{config.PENTEST_DATASET_NAME}' → {resolved_dataset_id}")
                else:
                    print(f"[Thread: {thread_name}][!] Could not resolve dataset name '{config.PENTEST_DATASET_NAME}'")
            except Exception as e:
                print(f"[Thread: {thread_name}][!] Error resolving dataset name: {e}")
        elif config.PENTEST_DATASET_ID:
            resolved_dataset_id = config.PENTEST_DATASET_ID
            print(f"[Thread: {thread_name}][i] Using configured dataset ID: {resolved_dataset_id}")
        elif config.PENTEST_DATASET_NAME and not project_id:
            print(f"[Thread: {thread_name}][!] Dataset name provided but no project_id available for resolution")
            print(f"[Thread: {thread_name}][!] Tip: Ensure inventory scope includes a project or use PENTEST_DATASET_ID instead")
        else:
            print(f"[Thread: {thread_name}][!] Dataset enabled but no PENTEST_DATASET_ID or PENTEST_DATASET_NAME provided")

        if resolved_dataset_id:
            desired_dataset_id = resolved_dataset_id

    # Apply updates. If setting 2+ fields, do ONE PATCH to avoid timing/stale-read clobbering.
    requested_fields = sum(1 for v in [desired_system_prompt, desired_dataset_id, desired_system_description] if v)
    if requested_fields:
        try:
            if requested_fields >= 2:
                print(f"[Thread: {thread_name}][i] Configuring resource additional config (single PATCH)...")
                api.patch_llm_endpoint_additional_config(
                    jwt_token=jwt_token,
                    resource_instance_id=resource_id,
                    system_prompt=desired_system_prompt if desired_system_prompt else api._UNSET,
                    dataset_id=desired_dataset_id if desired_dataset_id else api._UNSET,
                    system_description=desired_system_description if desired_system_description else api._UNSET,
                )
                system_prompt_configured = bool(desired_system_prompt)
                dataset_configured = bool(desired_dataset_id)
                system_description_configured = bool(desired_system_description)
                print(f"[Thread: {thread_name}][✓] Resource additional config configured successfully")
            else:
                if desired_system_prompt:
                    print(f"[Thread: {thread_name}][i] Configuring system prompt on resource...")
                    api.configure_llm_endpoint_system_prompt(jwt_token, resource_id, desired_system_prompt)
                    system_prompt_configured = True
                    print(f"[Thread: {thread_name}][✓] System prompt configured successfully")

                if desired_dataset_id:
                    api.configure_llm_endpoint_dataset(jwt_token, resource_id, desired_dataset_id)
                    dataset_configured = True
                    print(f"[Thread: {thread_name}][✓] Dataset configured successfully")

                if desired_system_description:
                    api.configure_llm_endpoint_system_description(jwt_token, resource_id, desired_system_description)
                    system_description_configured = True
                    print(f"[Thread: {thread_name}][✓] System description configured successfully")
        except Exception as e:
            print(f"[Thread: {thread_name}][!] Warning: Failed to configure resource additional config: {e}")

    # Step 1: Check if we have a model mapping for this resource type

    pentest_connection_details = {}
    
    if resource_type and resource_type in config.PENTEST_MODEL_MAPPING:
        preferred_model = config.PENTEST_MODEL_MAPPING[resource_type]
        print(f"[Thread: {thread_name}][i] Model mapping found for {resource_type}: {preferred_model}")
        
        available_models = api.get_llm_pentest_models(jwt_token, resource_id)
        
        if available_models and preferred_model in available_models:
            pentest_connection_details["model"] = preferred_model
            print(f"[Thread: {thread_name}][✓] Using mapped model: {preferred_model}")
        elif available_models:
            print(f"[Thread: {thread_name}][!] Mapped model '{preferred_model}' not available")
            print(f"[Thread: {thread_name}][i] Available: {', '.join(available_models[:5])}{'...' if len(available_models) > 5 else ''}")
            print(f"[Thread: {thread_name}][i] Using endpoint default")
        else:
            print(f"[Thread: {thread_name}][!] Could not fetch available models")
            print(f"[Thread: {thread_name}][i] Using endpoint default")
    else:
        if resource_type:
            print(f"[Thread: {thread_name}][i] No model mapping for {resource_type}; using endpoint default")
        else:
            print(f"[Thread: {thread_name}][i] Resource type unknown; using endpoint default")
    
    # Build request payload
    data = {
        "resource_instance_id": resource_id,
        "llm_pentest_scan_template_id": template_id,
        "description": f"CI/CD Pentest for {resource_name}",
        "pentest_connection_details": pentest_connection_details,
        "system_prompt_enabled": config.PENTEST_SYSTEM_PROMPT_ENABLED,
        "apply_guardrails": config.PENTEST_APPLY_GUARDRAILS,
        "num_attempts_on_testcase": config.PENTEST_NUM_ATTEMPTS,
    }
    
    print(f"[Thread: {thread_name}][i] System prompt enabled: {config.PENTEST_SYSTEM_PROMPT_ENABLED}")
    print(f"[Thread: {thread_name}][i] Apply guardrails: {config.PENTEST_APPLY_GUARDRAILS}")
    print(f"[Thread: {thread_name}][i] Attempts per test case: {config.PENTEST_NUM_ATTEMPTS}")
    if config.PENTEST_DATASET_ENABLED and dataset_configured:
        print(f"[Thread: {thread_name}][i] Dataset configured: {resolved_dataset_id}")

    # Step 2: Start Pentest
    print(f"\n[Thread: {thread_name}][3] Starting pentest for {resource_name}...")
    start_api = f"/v2/llm-pentest/customer/{config.CUSTOMER_ID}/start-pentest"

    try:
        resp = api.make_api_request(start_api, token=jwt_token, method="POST", data=data)
        resp_json = resp.json()
        job_id = resp_json["job_id"]
        scan_execution_id = resp_json["llm_pentest_scan_execution_id"]
        print(f"[Thread: {thread_name}][+] Pentest Execution ID: {scan_execution_id}")
        print(f"[Thread: {thread_name}][+] Pentest Job ID: {job_id}")
    except Exception as e:
        print(f"[Thread: {thread_name}][-] Failed to start pentest for {resource_name}: {e}")
        
        if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
            _cleanup_system_prompt(jwt_token, resource_id, thread_name)
        if dataset_configured and config.PENTEST_CLEANUP_DATASET:
            _cleanup_dataset(jwt_token, resource_id, thread_name)
        if system_description_configured and config.PENTEST_CLEANUP_RESOURCE_SYSTEM_DESCRIPTION:
            _cleanup_system_description(jwt_token, resource_id, thread_name)
        
        if is_retryable_start_error(e):
            return {"resource_id": resource_id, "resource_name": resource_name, "status": "START_FAILED_RETRYABLE", "error": str(e)}
        else:
            return {"resource_id": resource_id, "resource_name": resource_name, "status": "START_FAILED", "error": str(e)}
        
    # Step 3: Poll GraphQL for execution completion (source of truth)
    print(f"\n[Thread: {thread_name}][3] Polling GraphQL for execution completion...")

    # Polling controls
    GRAPHQL_POLL_INTERVAL = config.GRAPHQL_POLL_INTERVAL_SECS
    POLL_TIMEOUT_SECS = config.POLL_TIMEOUT_SECS
    POLL_STATUS_LOG_EVERY = config.POLL_STATUS_LOG_EVERY
    POLL_TIMEOUT_ACTION = config.POLL_TIMEOUT_ACTION

    start_ts = time.time()
    poll_counter = 0
    outcome: Optional[str] = None

    while True:
        elapsed = time.time() - start_ts
        if elapsed >= POLL_TIMEOUT_SECS:
            timeout_msg = f"GraphQL polling timed out after {int(POLL_TIMEOUT_SECS)}s for {resource_name}."
            print(f"[Thread: {thread_name}][-] {timeout_msg}")

            if POLL_TIMEOUT_ACTION == "continue":
                print(f"[Thread: {thread_name}][!] Continuing without results (POLL_TIMEOUT_ACTION=continue)...")
                
                if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                    _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                if dataset_configured and config.PENTEST_CLEANUP_DATASET:
                    _cleanup_dataset(jwt_token, resource_id, thread_name)
                
                return {
                    "resource_id": resource_id,
                    "resource_name": resource_name,
                    "status": "POLL_TIMEOUT_CONTINUE",
                    "outcome": "Unknown",
                    "message": f"Timed out after {int(POLL_TIMEOUT_SECS)} seconds, pentest may still be running",
                    "scan_execution_id": scan_execution_id,
                    "job_id": job_id,
                }

            elif POLL_TIMEOUT_ACTION == "partial":
                print(f"[Thread: {thread_name}][!] Checking for partial results (POLL_TIMEOUT_ACTION=partial)...")
                # One final attempt to get outcome
                try:
                    data = api.query_pentest_execution_full(jwt_token, scan_execution_id) or {}
                    exec_info = (data.get("llmPentestScanExecution") or {}) if isinstance(data, dict) else {}
                    outcome = (exec_info.get("outcomeLevel") or "").strip()

                    if outcome:
                        print(f"[Thread: {thread_name}][+] Found outcome during timeout check: {outcome}")
                        api.download_results_csv(jwt_token, resource_name, resource_id, scan_execution_id)
                        
                        if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                            _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                        if dataset_configured and config.PENTEST_CLEANUP_DATASET:
                            _cleanup_dataset(jwt_token, resource_id, thread_name)
                        
                        return {
                            "resource_id": resource_id,
                            "resource_name": resource_name,
                            "status": "COMPLETED",
                            "outcome": outcome,
                            "scan_execution_id": scan_execution_id,
                            "job_id": job_id,
                            "message": "Completed during timeout check"
                        }
                except Exception as e:
                    print(f"[Thread: {thread_name}][-] Error checking for partial results: {e}")

            print(f"[Thread: {thread_name}][-] {timeout_msg}")
            
            if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                _cleanup_system_prompt(jwt_token, resource_id, thread_name)
            if dataset_configured and config.PENTEST_CLEANUP_DATASET:
                _cleanup_dataset(jwt_token, resource_id, thread_name)
            
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "POLL_TIMEOUT",
                "error": f"Timed out after {int(POLL_TIMEOUT_SECS)} seconds",
                "scan_execution_id": scan_execution_id,
                "job_id": job_id,
            }

        try:
            # Query GraphQL for actual execution status
            data = api.query_pentest_execution_full(jwt_token, scan_execution_id) or {}
            exec_info = (data.get("llmPentestScanExecution") or {}) if isinstance(data, dict) else {}
            outcome = (exec_info.get("outcomeLevel") or "").strip()

            poll_counter += 1

            # If we have an outcome, the scan is truly complete
            if outcome:
                elapsed_time = time.time() - start_ts
                print(f"[Thread: {thread_name}][+] Scan completed after {elapsed_time:.1f}s")
                print(f"[Thread: {thread_name}][+] Outcome: {outcome}")
                break

            # Log periodically to show progress
            if poll_counter % POLL_STATUS_LOG_EVERY == 0:
                print(f"[Thread: {thread_name}]    Still processing... (elapsed: {elapsed:.1f}s, poll #{poll_counter})")

            time.sleep(GRAPHQL_POLL_INTERVAL)

        except requests.HTTPError as e:
            code = getattr(e.response, "status_code", None)
            if code in (401, 403):
                # Permission denied - fail immediately
                print(f"[Thread: {thread_name}][-] GraphQL permission denied ({code})")
                
                if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
                    _cleanup_system_prompt(jwt_token, resource_id, thread_name)
                if dataset_configured and config.PENTEST_CLEANUP_DATASET:
                    _cleanup_dataset(jwt_token, resource_id, thread_name)

                return {
                    "resource_id": resource_id,
                    "resource_name": resource_name,
                    "status": "GRAPHQL_PERMISSION_DENIED",
                    "error": f"GraphQL llmPentestScanExecution permission denied: {code}",
                    "scan_execution_id": scan_execution_id,
                    "job_id": job_id,
                }
            
            # Other HTTP errors - log and retry
            print(f"[Thread: {thread_name}][!] GraphQL HTTP error {code}, will retry...")
            time.sleep(GRAPHQL_POLL_INTERVAL)

        except Exception as e:
            # Network errors or other exceptions - log and retry
            print(f"[Thread: {thread_name}][!] GraphQL polling error: {e}, will retry...")
            time.sleep(GRAPHQL_POLL_INTERVAL)

    # Execution completed - download CSV
    print(f"[Thread: {thread_name}][+] Pentest completed for {resource_name}. Downloading results...")
    api.download_results_csv(jwt_token, resource_name, resource_id, scan_execution_id)

    # Cleanup resources
    if system_prompt_configured and config.PENTEST_CLEANUP_SYSTEM_PROMPT:
        _cleanup_system_prompt(jwt_token, resource_id, thread_name)
    if dataset_configured and config.PENTEST_CLEANUP_DATASET:
        _cleanup_dataset(jwt_token, resource_id, thread_name)
    if system_description_configured and config.PENTEST_CLEANUP_RESOURCE_SYSTEM_DESCRIPTION:
        _cleanup_system_description(jwt_token, resource_id, thread_name)

    # Return success with outcome from GraphQL polling
    return {
        "resource_id": resource_id,
        "resource_name": resource_name,
        "status": "COMPLETED",
        "outcome": outcome,
        "scan_execution_id": scan_execution_id,
        "job_id": job_id,
    }


def _cleanup_system_prompt(jwt_token: str, resource_id: str, thread_name: str) -> None:
    """Helper to cleanup system prompt from resource after pentest (used in multiple exit paths)."""
    try:
        api.cleanup_llm_endpoint_system_prompt(jwt_token, resource_id)
        print(f"[Thread: {thread_name}][i] System prompt cleaned up from resource")
    except Exception as e:
        print(f"[Thread: {thread_name}][!] Warning: Failed to cleanup system prompt: {e}")


def _cleanup_dataset(jwt_token: str, resource_id: str, thread_name: str) -> None:
    """Helper to cleanup dataset from resource after pentest (used in multiple exit paths)."""
    try:
        api.cleanup_llm_endpoint_dataset(jwt_token, resource_id)
        print(f"[Thread: {thread_name}][i] Dataset cleaned up from resource")
    except Exception as e:
        print(f"[Thread: {thread_name}][!] Warning: Failed to cleanup dataset: {e}")

def _cleanup_system_description(jwt_token: str, resource_id: str, thread_name: str) -> None:
    try:
        api.cleanup_llm_endpoint_system_description(jwt_token, resource_id)
        print(f"[Thread: {thread_name}][i] System description cleaned up from resource")
    except Exception as e:
        print(f"[Thread: {thread_name}][!] Warning: Failed to cleanup system description: {e}")


# ---------------------------


# ---------------------------
# Rolling parallel with start-retry queue
# ---------------------------

class RetryableResource:
    def __init__(
        self, 
        resource_id: str, 
        resource_name: str, 
        resource_type: Optional[str] = None,
        project_id: Optional[str] = None,
        attempt: int = 1, 
        last_error: Optional[str] = None
    ):
        self.resource_id = resource_id
        self.resource_name = resource_name
        self.resource_type = resource_type
        self.project_id = project_id
        self.attempt = attempt
        self.last_error = last_error
        self.next_retry_time = time.time() + config.START_RETRY_DELAY if attempt > 1 else 0

    def can_retry(self) -> bool:
        return self.attempt <= config.MAX_START_RETRIES and time.time() >= self.next_retry_time

    def increment_attempt(self, error: Optional[str] = None):
        self.attempt += 1
        self.last_error = error
        if self.attempt <= config.MAX_START_RETRIES:
            self.next_retry_time = time.time() + config.START_RETRY_DELAY

def run_rolling_parallel_with_retry(
    resource_ids: List[str],
    resource_mapping: Dict[str, str],
    resource_type_mapping: Dict[str, str],
    template_id: str,
    max_concurrent: int,
    project_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    all_results: List[Dict[str, Any]] = []

    resource_queue: deque[RetryableResource] = deque([
        RetryableResource(
            rid, 
            resource_mapping[rid], 
            resource_type_mapping.get(rid),
            project_id
        ) for rid in resource_ids
    ])
    retry_queue: deque[RetryableResource] = deque()

    active_futures: Dict[Any, tuple[RetryableResource, float]] = {}
    completed_count = 0

    # remember when we last issued a *start* API call
    last_start_ts: float = 0.0

    print(f"\n{'='*80}")
    print(f"STARTING ROLLING PARALLEL PENTESTS WITH RETRY")
    print(f"Total resources: {len(resource_ids)}")
    print(f"Max concurrent: {max_concurrent}")
    print(f"Max start retries per resource: {config.MAX_START_RETRIES}")
    print(f"Start retry delay: {config.START_RETRY_DELAY}s")
    if project_id:
        print(f"Project ID (for dataset resolution): {project_id}")
    print(f"{'='*80}")

    with ThreadPoolExecutor(max_workers=max_concurrent, thread_name_prefix="PentestWorker") as executor:

        def get_next_resource() -> Optional[RetryableResource]:
            if retry_queue:
                ready: list[RetryableResource] = []
                not_ready: deque[RetryableResource] = deque()
                while retry_queue:
                    rr = retry_queue.popleft()
                    if rr.can_retry():
                        ready.append(rr)
                    else:
                        not_ready.append(rr)
                retry_queue.extend(not_ready)
                if ready:
                    next_rr = ready[0]
                    for rr in reversed(ready[1:]):
                        retry_queue.appendleft(rr)
                    return next_rr
            if resource_queue:
                return resource_queue.popleft()
            return None

        def start_next_pentest() -> bool:
            """Start the next available pentest if we have capacity, respecting START_STAGGER_SECS."""
            nonlocal last_start_ts

            if len(active_futures) >= max_concurrent:
                return False

            next_resource = get_next_resource()
            if not next_resource:
                return False

            # ---- stagger starts to avoid backend spikes ----
            if config.START_STAGGER_SECS > 0:
                now = time.time()
                wait = config.START_STAGGER_SECS - (now - last_start_ts)
                if wait > 0:
                    # Short sleep (split large waits so the loop remains responsive)
                    sleep_for = min(wait, 2.0)
                    print(f"[ROLLING] Staggering next start by {sleep_for:.1f}s…")
                    time.sleep(sleep_for)
                    # Re-evaluate once (good enough for whole seconds)
                    now = time.time()
                    wait = config.START_STAGGER_SECS - (now - last_start_ts)
                    if wait > 0:
                        time.sleep(wait)

            retry_info = f" (retry {next_resource.attempt}/{config.MAX_START_RETRIES})" if next_resource.attempt > 1 else ""
            print(f"\n[ROLLING] Starting pentest {len(active_futures) + 1}: {next_resource.resource_name}{retry_info}")
            if next_resource.last_error:
                print(f"          Previous error: {next_resource.last_error}")

            # Mark the moment we kick off a *start* call
            last_start_ts = time.time()

            future = executor.submit(
                run_pentest_for_resource,
                next_resource.resource_id,
                next_resource.resource_name,
                template_id,
                next_resource.resource_type,
                next_resource.project_id
            )
            active_futures[future] = (next_resource, time.time())
            return True

        def fill_available_slots() -> int:
            started_count = 0
            while start_next_pentest():
                started_count += 1
            return started_count

        initial_started = fill_available_slots()
        print(f"[ROLLING] Started {initial_started} initial pentests")

        while active_futures or resource_queue or retry_queue:
            if not active_futures:
                min_wait_time = float('inf')
                for retry_resource in list(retry_queue):
                    if retry_resource.can_retry():
                        if fill_available_slots() > 0:
                            break
                    else:
                        wait_time = retry_resource.next_retry_time - time.time()
                        if wait_time > 0:
                            min_wait_time = min(min_wait_time, wait_time)
                if not active_futures:
                    if min_wait_time != float('inf') and min_wait_time > 0:
                        print(f"[ROLLING] Waiting {min_wait_time:.1f}s for next retry opportunity...")
                        time.sleep(min(min_wait_time, 5))
                        continue
                    else:
                        break

            if len(active_futures) < max_concurrent:
                started_now = fill_available_slots()
                if started_now > 0:
                    print(f"[ROLLING] Started {started_now} pentest(s) to fill available slots (proactive)")

            try:
                from concurrent.futures import as_completed, TimeoutError as FuturesTimeoutError
                for future in as_completed(list(active_futures.keys()), timeout=1):
                    retry_resource, start_time = active_futures[future]
                    duration = time.time() - start_time
                    completed_count += 1
                    try:
                        result = future.result()
                        if result.get('status') == 'START_FAILED_RETRYABLE':
                            retry_resource.increment_attempt(result.get('error'))
                            if retry_resource.attempt <= config.MAX_START_RETRIES:
                                retry_queue.append(retry_resource)
                                print(f"\n[ROLLING] Start failed ({completed_count}): {retry_resource.resource_name}")
                                print(f"          Error: {result.get('error')}")
                                print(f"          Queued for retry {retry_resource.attempt}/{config.MAX_START_RETRIES} in {config.START_RETRY_DELAY}s")
                            else:
                                print(f"\n[ROLLING] Start failed permanently ({completed_count}): {retry_resource.resource_name}")
                                print(f"          Exceeded max retries ({config.MAX_START_RETRIES})")
                                result['status'] = 'START_FAILED_MAX_RETRIES'
                                result['final_attempt'] = retry_resource.attempt - 1
                                all_results.append(result)
                        else:
                            all_results.append(result)
                            status = result.get('status', 'Unknown')
                            outcome = result.get('outcome', 'N/A')
                            errorish = {
                                "START_FAILED", "START_FAILED_MAX_RETRIES", "PENTEST_FAILED", "EXCEPTION",
                                "POLL_TIMEOUT", "EXTENDED_POLL_TIMEOUT",
                                "GRAPHQL_PERMISSION_DENIED", "POLL_PERMISSION_DENIED",
                                "NO_EXECUTION_DATA", "EVALUATION_FAILED",
                            }
                            label = "Finished with error" if status in errorish else "Completed"
                            retry_info = f" (after {retry_resource.attempt} attempts)" if retry_resource.attempt > 1 else ""
                            print(f"\n[ROLLING] {label} ({completed_count}): {retry_resource.resource_name}{retry_info}")
                            print(f"          Status: {status}, Outcome: {outcome}, Duration: {duration:.1f}s")
                    except Exception as e:
                        print(f"\n[ROLLING] Exception ({completed_count}): {retry_resource.resource_name} - {e}")
                        all_results.append({
                            "resource_id": retry_resource.resource_id,
                            "resource_name": retry_resource.resource_name,
                            "status": "EXCEPTION",
                            "error": str(e),
                        })
                    del active_futures[future]
                    new_started = fill_available_slots()
                    if new_started > 0:
                        print(f"          Started {new_started} new pentest(s) to fill available slots")
                    break
            except FuturesTimeoutError:
                ready_retries: list[RetryableResource] = []
                remaining_retries: list[RetryableResource] = []
                while retry_queue:
                    retry_resource = retry_queue.popleft()
                    if retry_resource.can_retry():
                        ready_retries.append(retry_resource)
                    elif retry_resource.attempt <= config.MAX_START_RETRIES:
                        remaining_retries.append(retry_resource)
                for rr in reversed(remaining_retries):
                    retry_queue.appendleft(rr)
                started_retries = 0
                for retry_resource in ready_retries:
                    if len(active_futures) < max_concurrent:
                        retry_info = f" (retry {retry_resource.attempt}/{config.MAX_START_RETRIES})"
                        print(f"\n[ROLLING] Starting retry: {retry_resource.resource_name}{retry_info}")
                        if retry_resource.last_error:
                            print(f"          Previous error: {retry_resource.last_error}")
                        future = executor.submit(
                            run_pentest_for_resource, 
                            retry_resource.resource_id, 
                            retry_resource.resource_name, 
                            template_id, 
                            retry_resource.resource_type,
                            retry_resource.project_id
                        )
                        active_futures[future] = (retry_resource, time.time())
                        started_retries += 1
                    else:
                        retry_queue.appendleft(retry_resource)
                        break
                if started_retries > 0:
                    print(f"          Started {started_retries} retry pentest(s)")
                continue

    print(f"\n[ROLLING] All pentests completed! Total: {len(all_results)} results")

    while retry_queue:
        retry_resource = retry_queue.popleft()
        print(f"[ROLLING] Resource {retry_resource.resource_name} exceeded max retries ({config.MAX_START_RETRIES})")
        all_results.append({
            "resource_id": retry_resource.resource_id,
            "resource_name": retry_resource.resource_name,
            "status": "START_FAILED_MAX_RETRIES",
            "error": retry_resource.last_error,
            "final_attempt": retry_resource.attempt,
        })

    return all_results