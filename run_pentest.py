import os
import re
import time
import json
import requests
import random
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError
from typing import List, Dict, Any
from collections import deque

# ==============================
# Config
# ==============================
from dotenv import load_dotenv

# Load .env file only if running locally
if os.getenv("CI") != "true":  # GitHub Actions sets CI=true
    load_dotenv()

API_URL = os.environ.get("API_URL")
API_KEY = os.environ.get("API_KEY")
CUSTOMER_ID = os.environ.get("CUSTOMER_ID")
ORGANIZATION_ID = os.environ.get("ORGANIZATION_ID")

if not all([API_URL, API_KEY, CUSTOMER_ID, ORGANIZATION_ID]):
    raise ValueError("Missing one or more required endpoint environment variables.")

# Parallel execution controls
MAX_CONCURRENT_PENTESTS = int(os.getenv("MAX_CONCURRENT_PENTESTS", "3"))

# Retry configuration for failed starts
MAX_START_RETRIES = int(os.getenv("MAX_START_RETRIES", "3"))
START_RETRY_DELAY = int(os.getenv("START_RETRY_DELAY", "30"))  # seconds between retries

print(f"[+] Configuration: MAX_CONCURRENT_PENTESTS={MAX_CONCURRENT_PENTESTS}")
print(f"[+] Configuration: MAX_START_RETRIES={MAX_START_RETRIES}")
print(f"[+] Configuration: START_RETRY_DELAY={START_RETRY_DELAY}")

# ==============================
# Auth
# ==============================

# Thread-local storage for JWT tokens (in case they need to be refreshed)
thread_local = threading.local()

def get_jwt_token(api_key: str) -> str:
    endpoint = f"{API_URL}/v1/auth/issue-jwt-token"
    headers = {"X-API-Key": api_key, "Accept": "application/json"}
    resp = requests.post(endpoint, headers=headers)
    resp.raise_for_status()
    return resp.json()["access_token"]


def get_thread_jwt_token() -> str:
    """Get JWT token for current thread, creating one if needed"""
    if not hasattr(thread_local, 'jwt_token'):
        thread_local.jwt_token = get_jwt_token(API_KEY)
        if os.getenv("LOG_JWT_THREADS", "false").lower() == "true":
            print(f"[+] JWT token obtained for thread {threading.current_thread().name}")
    return thread_local.jwt_token


def _mask(s: str, show: int = 4) -> str:
    if not s:
        return s
    return s[:show] + "…" if len(s) > show else "…"


def make_api_request(
    endpoint: str,
    token: str,
    method: str = "GET",
    data=None,
    params=None,
    include_api_key: bool = False,
    accept: str = "application/json",
    content_type: str | None = "application/json",
    timeout: int | float | None = None,
) -> requests.Response:
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": accept,
    }
    if content_type:
        headers["Content-Type"] = content_type
    if include_api_key:
        headers["X-API-Key"] = API_KEY

    url = f"{API_URL}{endpoint}"
    resp = requests.request(method, url, headers=headers, params=params, json=data, timeout=timeout)
    try:
        resp.raise_for_status()
    except requests.HTTPError:
        # Print helpful but safe diagnostics
        safe_headers = {
            k: (v if k.lower() not in {"authorization", "x-api-key"} else _mask(v))
            for k, v in headers.items()
        }
        print(f"[-] {method} {url} failed: {resp.status_code}")
        print(f"    Headers sent: {safe_headers}")
        print(f"    Response: {resp.text}")
        raise
    return resp


def sanitize_name(name: str) -> str:
    """Allow alnum, space, dash, underscore; then convert spaces to underscores and collapse repeats."""
    safe = "".join(c for c in name if c.isalnum() or c in (" ", "-", "_")).rstrip()
    safe = safe.replace(" ", "_")
    # collapse multiple underscores
    safe = re.sub(r"_+", "_", safe)
    return safe


def download_results_csv(jwt_token: str, resource_name: str, resource_id: str, scan_execution_id: str) -> str | None:
    """Download CSV artifact for a scan execution via API and return the filename if saved."""
    download_api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/executions/{scan_execution_id}/download-csv"
    try:
        resp = make_api_request(
            download_api,
            token=jwt_token,
            method="POST",
            data=None,
            params=None,
            include_api_key=False,
            accept="text/csv,application/octet-stream,*/*",
            content_type=None,  # omit Content-Type for raw download
            timeout=60,
        )
        # Save bytes
        safe_name = sanitize_name(resource_name)
        filename = f"pentest_results_{safe_name}_{resource_id[:8]}.csv"
        with open(filename, "wb") as f:
            f.write(resp.content)
        print(f"[Thread: {threading.current_thread().name}][+] CSV saved as {filename}")
        return filename
    except Exception as e:
        print(f"[Thread: {threading.current_thread().name}][-] Error downloading results: {e}")
        return None


def query_execution(jwt_token: str, scan_execution_id: str, full: bool = True) -> Dict[str, Any] | None:
    """Query GraphQL for execution details; raises on GraphQL errors, returns execution dict or None."""
    if full:
        graphql_query = """
        query PentestExecution($customerId: UUID!, $execId: UUID!) {
          llmPentestScanExecution(
            filter: {
              customerId: $customerId,
              llmPentestScanExecutionId: $execId
            }
          ) {
            taskId
            finishedAt
            description
            outcomeLevel
            chosenLlmModel
            reportGenerationJobId
          }
        }
        """
    else:
        graphql_query = """
        query PentestExecution($customerId: UUID!, $execId: UUID!) {
          llmPentestScanExecution(
            filter: {
              customerId: $customerId,
              llmPentestScanExecutionId: $execId
            }
          ) {
            outcomeLevel
            finishedAt
          }
        }
        """

    variables = {"customerId": CUSTOMER_ID, "execId": scan_execution_id}
    resp = make_api_request(
        "/v2/graphql",
        token=jwt_token,
        method="POST",
        data={"query": graphql_query, "variables": variables},
        accept="application/json",
        content_type="application/json",
        timeout=30,
    )
    gql = resp.json()
    if "errors" in gql:
        raise RuntimeError(gql["errors"])
    return gql.get("data", {}).get("llmPentestScanExecution")


# Get initial JWT token for main thread
JWT_TOKEN = get_jwt_token(API_KEY)
print("[+] JWT token obtained successfully.")

# ==============================
# 1. Get Existing LLM Endpoints
# ==============================
print("\n[1] Fetching existing LLM endpoints...")
api = f"/v1/inventory/customer/{CUSTOMER_ID}/resources"
params = {
    "organization": ORGANIZATION_ID,
    "resource_category": "llm_endpoint",
    "omit_not_ai": "true",
}

resp = make_api_request(api, token=JWT_TOKEN, method="GET", params=params)
resources = resp.json()["resources"]

print(f"[+] Found {len(resources)} LLM endpoints")
# Filter for only resources with valid pentest connection details
valid_resources = [r for r in resources if r.get("has_valid_pentest_connection_details", False)]
print(f"[+] {len(valid_resources)} endpoints have valid pentest connection details")

# Create a mapping of resource_id to resource_name for easier tracking
resource_mapping = {}
for resource in valid_resources:
    resource_id = resource["resource_instance_id"]
    resource_name = resource["resource_display_name"]
    resource_mapping[resource_id] = resource_name
    print(f"    - {resource_name}")
    print(f"      ID: {resource_id}")
    print()

SELECTED_RESOURCE_IDS = list(resource_mapping.keys())
print(f"[+] Total selected resources: {len(SELECTED_RESOURCE_IDS)}")

# ==============================
# 2. Select Pentest Template
# ==============================
print("\n[2] Listing pentest templates...")
api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/templates"
resp = make_api_request(api, token=JWT_TOKEN, method="GET")
templates = resp.json()["llm_pentest_scan_templates"]

# Select template by name
TARGET_TEMPLATE_NAME = os.environ.get("TARGET_TEMPLATE_NAME", "CI/CD Pentest All Categories")
pentest_template_id = None

for template in templates:
    if template["name"] == TARGET_TEMPLATE_NAME:
        pentest_template_id = template["llm_pentest_scan_template_id"]
        print(f"[+] Found template '{TARGET_TEMPLATE_NAME}' with ID: {pentest_template_id}")
        break

# ===== Fail-fast guard if template not found =====
if pentest_template_id is None:
    print(f"❌ Pentest template '{TARGET_TEMPLATE_NAME}' was not found.")
    available = ", ".join(t.get("name", "<unnamed>") for t in templates) or "(no templates returned)"
    print(f"    Available templates: {available}")
    print("    Tip: Set TARGET_TEMPLATE_NAME env var to one of the available templates, or create the template in the UI/API.")
    exit(1)
# ================================================

print(f"[+] Using Pentest Template ID: {pentest_template_id}")

# ==============================
# Helper Functions for Steps 3-6
# ==============================

def is_retryable_start_error(e: Exception) -> bool:
    """Determine if a pentest start error is retryable"""
    if isinstance(e, requests.HTTPError):
        status_code = e.response.status_code
        # Retry on server errors (5xx) and some client errors
        if 500 <= status_code < 600:
            return True
        # Retry on rate limiting
        if status_code == 429:
            return True
        # Don't retry on auth errors, bad request with validation errors, etc.
        if status_code in [401, 403]:
            return False
        # For 400 errors, check if it's a validation error vs temporary issue
        if status_code == 400:
            try:
                error_text = e.response.text.lower()
                # Don't retry validation errors
                if any(keyword in error_text for keyword in ['validation error', 'invalid api key', 'exceeded your current quota']):
                    return False
                # Retry other 400s as they might be temporary
                return True
            except:
                return True
    return True


def run_pentest_for_resource(resource_id: str, resource_name: str, template_id: str) -> dict:
    """Run complete pentest workflow for a single resource (thread-safe)"""
    thread_name = threading.current_thread().name
    print(f"\n{'='*60}")
    print(f"[Thread: {thread_name}] Starting pentest for: {resource_name}")
    print(f"[Thread: {thread_name}] Resource ID: {resource_id}")
    print(f"{'='*60}")

    # Get thread-local JWT token
    jwt_token = get_thread_jwt_token()

    # Step 3: Start Pentest (with retry logic handled at higher level)
    print(f"\n[Thread: {thread_name}][3] Starting pentest for {resource_name}...")
    api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/start-pentest"
    data = {
        "resource_instance_id": resource_id,
        "llm_pentest_scan_template_id": template_id,
        "description": f"CI/CD Pentest for {resource_name}",
        "pentest_connection_details": {},
    }

    try:
        resp = make_api_request(api, token=jwt_token, method="POST", data=data)
        resp_json = resp.json()

        job_id = resp_json["job_id"]
        scan_execution_id = resp_json["llm_pentest_scan_execution_id"]
        print(f"[Thread: {thread_name}][+] Pentest Execution ID: {scan_execution_id}")
        print(f"[Thread: {thread_name}][+] Pentest Job ID: {job_id}")
    except Exception as e:
        print(f"[Thread: {thread_name}][-] Failed to start pentest for {resource_name}: {e}")
        # Return special status for retryable errors
        if is_retryable_start_error(e):
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "START_FAILED_RETRYABLE",
                "error": str(e),
            }
        else:
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "START_FAILED",
                "error": str(e),
            }

    # Step 4: Poll for Pentest Results
    print(f"\n[Thread: {thread_name}][4] Checking pentest execution status for {resource_name}...")
    job_status_api = f"/v2/llm-pentest/job-status/{job_id}"

    # --- Polling controls (tweak via env) ---
    POLL_INTERVAL_SECS = float(os.getenv("POLL_INTERVAL_SECS", "60"))
    POLL_TIMEOUT_SECS = float(os.getenv("POLL_TIMEOUT_SECS", "7200"))
    BACKOFF_BASE_SECS = float(os.getenv("POLL_BACKOFF_BASE_SECS", "5"))
    BACKOFF_MAX_SECS = float(os.getenv("POLL_BACKOFF_MAX_SECS", "300"))
    NOT_FOUND_GRACE_POLLS = int(os.getenv("POLL_NOT_FOUND_GRACE", "3"))
    POLL_STATUS_LOG_EVERY = max(1, int(os.getenv("POLL_STATUS_LOG_EVERY", "10")))  # log every N polls even if unchanged

    # polling behavior options
    POLL_TIMEOUT_ACTION = os.getenv("POLL_TIMEOUT_ACTION", "fail").lower()  # "fail", "continue", or "partial"

    start_ts = time.time()
    status = None
    last_logged_status = None
    poll_counter = 0
    retry_streak = 0
    not_found_streak = 0

    while True:
        # Check overall timeout
        elapsed = time.time() - start_ts
        if elapsed >= POLL_TIMEOUT_SECS:
            timeout_msg = f"Polling timed out after {int(POLL_TIMEOUT_SECS)}s for {resource_name}."

            # If last known status was RUNNING, try extended GraphQL polling
            if status == "RUNNING":
                print(f"[Thread: {thread_name}][!] {timeout_msg} Last status was RUNNING - switching to GraphQL polling...")

                # Extended polling phase using GraphQL
                graphql_start = time.time()
                graphql_timeout = int(os.getenv("GRAPHQL_EXTENDED_TIMEOUT_SECS", "1800"))  # 30 min default
                graphql_interval = int(os.getenv("GRAPHQL_POLL_INTERVAL_SECS", "120"))     # 2 min default

                print(f"[Thread: {thread_name}][+] Starting extended GraphQL polling (timeout: {graphql_timeout}s, interval: {graphql_interval}s)")

                while (time.time() - graphql_start) < graphql_timeout:
                    try:
                        execution = query_execution(jwt_token, scan_execution_id, full=True)
                        if execution:
                            outcome = (execution.get("outcomeLevel") or "").strip()
                            finished_at = execution.get("finishedAt")

                            if finished_at and outcome:
                                elapsed_extended = time.time() - graphql_start
                                total_elapsed = time.time() - start_ts
                                print(f"[Thread: {thread_name}][+] Pentest completed during extended polling! (Extended: {elapsed_extended:.1f}s, Total: {total_elapsed:.1f}s)")
                                print(f"[Thread: {thread_name}][+] Outcome: {outcome}")

                                # Download results
                                download_results_csv(jwt_token, resource_name, resource_id, scan_execution_id)

                                return {
                                    "resource_id": resource_id,
                                    "resource_name": resource_name,
                                    "status": "COMPLETED",
                                    "outcome": outcome,
                                    "execution_details": execution,
                                    "scan_execution_id": scan_execution_id,
                                    "job_id": job_id,
                                    "total_duration": total_elapsed,
                                    "message": "Completed via extended GraphQL polling"
                                }

                            elif outcome:
                                print(f"[Thread: {thread_name}][+] Found outcome during extended polling: {outcome} (but no finishedAt)")

                        # Still running, wait and try again
                        print(f"[Thread: {thread_name}]    Extended GraphQL poll: still running (elapsed: {time.time() - graphql_start:.1f}s)")
                        time.sleep(graphql_interval)

                    except Exception as e:
                        print(f"[Thread: {thread_name}][!] GraphQL polling error: {e}")
                        time.sleep(min(graphql_interval, 60))  # Wait but not too long on errors

                # Extended polling timed out
                print(f"[Thread: {thread_name}][-] Extended GraphQL polling timed out after {graphql_timeout}s")
                return {
                    "resource_id": resource_id,
                    "resource_name": resource_name,
                    "status": "EXTENDED_POLL_TIMEOUT",
                    "message": f"Pentest still running after extended polling ({int(POLL_TIMEOUT_SECS + graphql_timeout)}s total)",
                    "scan_execution_id": scan_execution_id,
                    "job_id": job_id,
                }

            # timeout handling for non-RUNNING statuses
            if POLL_TIMEOUT_ACTION == "continue":
                print(f"[Thread: {thread_name}][!] {timeout_msg} Continuing without downloading results...")

                return {
                    "resource_id": resource_id,
                    "resource_name": resource_name,
                    "status": "POLL_TIMEOUT_CONTINUE",
                    "outcome": "Unknown",
                    "message": f"Timed out after {int(POLL_TIMEOUT_SECS)} seconds, pentest may still be running",
                    "scan_execution_id": scan_execution_id,
                    "job_id": job_id,
                }

            elif POLL_TIMEOUT_ACTION == "partial":
                print(f"[Thread: {thread_name}][!] {timeout_msg} Checking if any results available...")

                # Try GraphQL to see if we have any outcome yet
                try:
                    execution = query_execution(jwt_token, scan_execution_id, full=False)
                    if execution:
                        outcome = (execution.get("outcomeLevel") or "").strip()
                        finished_at = execution.get("finishedAt")

                        if outcome and finished_at:
                            print(f"[Thread: {thread_name}][+] Found completed results during timeout check!")
                            download_results_csv(jwt_token, resource_name, resource_id, scan_execution_id)
                            return {
                                "resource_id": resource_id,
                                "resource_name": resource_name,
                                "status": "COMPLETED",
                                "outcome": outcome,
                                "scan_execution_id": scan_execution_id,
                                "job_id": job_id,
                                "message": "Completed during timeout check"
                            }

                        elif outcome:
                            return {
                                "resource_id": resource_id,
                                "resource_name": resource_name,
                                "status": "POLL_TIMEOUT_PARTIAL",
                                "outcome": outcome,
                                "message": f"Partial outcome available: {outcome}",
                                "scan_execution_id": scan_execution_id,
                                "job_id": job_id,
                            }

                except Exception as e:
                    print(f"[Thread: {thread_name}][-] Error checking for partial results: {e}")

            # Default behavior - fail on timeout
            print(f"[Thread: {thread_name}][-] {timeout_msg}")
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "POLL_TIMEOUT",
                "error": f"Timed out after {int(POLL_TIMEOUT_SECS)} seconds",
                "scan_execution_id": scan_execution_id,
                "job_id": job_id,
            }

        try:
            resp = make_api_request(job_status_api, token=jwt_token, method="GET", include_api_key=True)
            resp_json = resp.json()
            status = resp_json.get("status")

            # Throttled logging: on change or every N polls
            poll_counter += 1
            should_log = False
            if status != last_logged_status:
                should_log = True
                last_logged_status = status
            elif status == "RUNNING" and (poll_counter % POLL_STATUS_LOG_EVERY == 0):
                should_log = True

            if should_log:
                print(f"[Thread: {thread_name}]    Current status for {resource_name}: {status}")

            # Any successful call resets backoff
            retry_streak = 0

            if status == "COMPLETED":
                break

            if status == "FAILED":
                print(f"[Thread: {thread_name}][-] Pentest failed for {resource_name}")
                return {"resource_id": resource_id, "resource_name": resource_name, "status": "PENTEST_FAILED"}

            if status == "NOT_FOUND":
                not_found_streak += 1
                if not_found_streak <= NOT_FOUND_GRACE_POLLS:
                    time.sleep(min(POLL_INTERVAL_SECS, 10))
                    continue
                if should_log is False:  # ensure we log the noteworthy NOT_FOUND after grace
                    print(f"[Thread: {thread_name}][!] job-status returned NOT_FOUND (streak={not_found_streak}); continuing to poll…")
            else:
                not_found_streak = 0

            time.sleep(POLL_INTERVAL_SECS)

        except requests.RequestException as e:
            retry_streak += 1
            code = getattr(getattr(e, "response", None), "status_code", None)

            raw_delay = BACKOFF_BASE_SECS * (2 ** min(retry_streak, 6))
            delay = min(BACKOFF_MAX_SECS, raw_delay) * (0.8 + 0.4 * random.random())

            print(
                f"[Thread: {thread_name}][!] Temporary polling error for {resource_name} (attempt {retry_streak}, status={code}). "
                f"Retrying in {delay:.1f}s. Error: {e}"
            )
            time.sleep(delay)

    # Download results
    print(f"[Thread: {thread_name}][+] Pentest completed for {resource_name}. Downloading results...")
    download_results_csv(jwt_token, resource_name, resource_id, scan_execution_id)

    # Step 5: GraphQL Outcome Evaluation
    print(f"\n[Thread: {thread_name}][5] Evaluating pentest outcome for {resource_name}...")

    try:
        execution = query_execution(jwt_token, scan_execution_id, full=True)
        if not execution:
            print(f"[Thread: {thread_name}][-] No llmPentestScanExecution in GraphQL response for {resource_name}")
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "NO_EXECUTION_DATA",
            }

        outcome = (execution.get("outcomeLevel") or "").strip()
        print(f"[Thread: {thread_name}][+] Outcome Level for {resource_name}: {outcome}")

        return {
            "resource_id": resource_id,
            "resource_name": resource_name,
            "status": "COMPLETED",
            "outcome": outcome,
            "execution_details": execution,
            "scan_execution_id": scan_execution_id,
            "job_id": job_id,
        }

    except Exception as e:
        print(f"[Thread: {thread_name}][-] Error evaluating outcome for {resource_name}: {e}")
        return {
            "resource_id": resource_id,
            "resource_name": resource_name,
            "status": "EVALUATION_FAILED",
            "error": str(e),
        }


class RetryableResource:
    """Container for resources that need retry logic"""
    def __init__(self, resource_id: str, resource_name: str, attempt: int = 1, last_error: str = None):
        self.resource_id = resource_id
        self.resource_name = resource_name
        self.attempt = attempt
        self.last_error = last_error
        self.next_retry_time = time.time() + START_RETRY_DELAY if attempt > 1 else 0

    def can_retry(self) -> bool:
        return self.attempt <= MAX_START_RETRIES and time.time() >= self.next_retry_time

    def increment_attempt(self, error: str = None):
        self.attempt += 1
        self.last_error = error
        if self.attempt <= MAX_START_RETRIES:
            self.next_retry_time = time.time() + START_RETRY_DELAY


def run_rolling_parallel_with_retry(resource_ids: List[str], resource_mapping: Dict[str, str], template_id: str, max_concurrent: int) -> List[Dict[str, Any]]:
    """Run pentests with rolling parallel execution and retry queue for failed starts"""
    all_results = []

    # Initialize queues
    resource_queue = deque([RetryableResource(rid, resource_mapping[rid]) for rid in resource_ids])
    retry_queue = deque()  # Resources that failed to start but can be retried

    active_futures = {}  # future -> (RetryableResource, start_time)
    completed_count = 0

    print(f"\n{'='*80}")
    print(f"STARTING ROLLING PARALLEL PENTESTS WITH RETRY")
    print(f"Total resources: {len(resource_ids)}")
    print(f"Max concurrent: {max_concurrent}")
    print(f"Max start retries per resource: {MAX_START_RETRIES}")
    print(f"Start retry delay: {START_RETRY_DELAY}s")
    print(f"{'='*80}")

    with ThreadPoolExecutor(max_workers=max_concurrent, thread_name_prefix="PentestWorker") as executor:

        def get_next_resource() -> RetryableResource | None:
            """Get the next resource to process, preferring retry queue for ready items"""
            # First check retry queue for resources ready to retry
            retry_ready = []

            # Collect all ready-to-retry resources
            while retry_queue:
                retry_resource = retry_queue.popleft()
                if retry_resource.can_retry():
                    retry_ready.append(retry_resource)
                elif retry_resource.attempt <= MAX_START_RETRIES:
                    # Still has retries left but not ready yet, put back
                    retry_queue.append(retry_resource)
                    break  # Don't check further since queue is time-ordered

            # Add ready retries back to front of retry queue (maintain order)
            retry_queue.extendleft(reversed(retry_ready))

            # Return first ready retry, or next fresh resource
            if retry_ready:
                return retry_queue.popleft()
            elif resource_queue:
                return resource_queue.popleft()
            else:
                return None

        def start_next_pentest() -> bool:
            """Start the next available pentest if we have capacity"""
            if len(active_futures) >= max_concurrent:
                return False

            next_resource = get_next_resource()
            if not next_resource:
                return False

            retry_info = f" (retry {next_resource.attempt}/{MAX_START_RETRIES})" if next_resource.attempt > 1 else ""
            print(f"\n[ROLLING] Starting pentest {len(active_futures) + 1}: {next_resource.resource_name}{retry_info}")

            if next_resource.last_error:
                print(f"          Previous error: {next_resource.last_error}")

            future = executor.submit(
                run_pentest_for_resource,
                next_resource.resource_id,
                next_resource.resource_name,
                template_id
            )
            active_futures[future] = (next_resource, time.time())
            return True

        def fill_available_slots():
            """Start pentests until we reach max capacity or run out of resources"""
            started_count = 0
            while start_next_pentest():
                started_count += 1
            return started_count

        # Start initial batch
        initial_started = fill_available_slots()
        print(f"[ROLLING] Started {initial_started} initial pentests")

        # Main processing loop
        while active_futures or resource_queue or retry_queue:
            if not active_futures:
                # No active futures but we have resources in queues
                # Check if any retries are ready
                min_wait_time = float('inf')

                for retry_resource in list(retry_queue):
                    if retry_resource.can_retry():
                        # Try to start it
                        if fill_available_slots() > 0:
                            break
                    else:
                        # Track minimum wait time
                        wait_time = retry_resource.next_retry_time - time.time()
                        if wait_time > 0:
                            min_wait_time = min(min_wait_time, wait_time)

                if not active_futures:
                    if min_wait_time != float('inf') and min_wait_time > 0:
                        print(f"[ROLLING] Waiting {min_wait_time:.1f}s for next retry opportunity...")
                        time.sleep(min(min_wait_time, 5))  # Cap wait at 5s for responsiveness
                        continue
                    else:
                        # No active futures and nothing to retry
                        break

            # Wait for completions
            try:
                for future in as_completed(active_futures.keys(), timeout=1):
                    retry_resource, start_time = active_futures[future]
                    duration = time.time() - start_time
                    completed_count += 1

                    try:
                        result = future.result()

                        # Handle retryable start failures
                        if result.get('status') == 'START_FAILED_RETRYABLE':
                            retry_resource.increment_attempt(result.get('error'))

                            if retry_resource.attempt <= MAX_START_RETRIES:
                                retry_queue.append(retry_resource)
                                print(f"\n[ROLLING] Start failed ({completed_count}): {retry_resource.resource_name}")
                                print(f"          Error: {result.get('error')}")
                                print(f"          Queued for retry {retry_resource.attempt}/{MAX_START_RETRIES} in {START_RETRY_DELAY}s")
                            else:
                                print(f"\n[ROLLING] Start failed permanently ({completed_count}): {retry_resource.resource_name}")
                                print(f"          Exceeded max retries ({MAX_START_RETRIES})")
                                result['status'] = 'START_FAILED_MAX_RETRIES'
                                result['final_attempt'] = retry_resource.attempt - 1
                                all_results.append(result)
                        else:
                            # Regular completion or non-retryable failure
                            all_results.append(result)
                            status = result.get('status', 'Unknown')
                            outcome = result.get('outcome', 'N/A')

                            errorish = {
                                "START_FAILED",
                                "START_FAILED_MAX_RETRIES",
                                "PENTEST_FAILED",
                                "EXCEPTION",
                                "POLL_TIMEOUT",
                                "EXTENDED_POLL_TIMEOUT",
                                "GRAPHQL_ERROR",
                                "NO_EXECUTION_DATA",
                                "EVALUATION_FAILED",
                            }
                            label = "Finished with error" if status in errorish else "Completed"
                            retry_info = f" (after {retry_resource.attempt} attempts)" if retry_resource.attempt > 1 else ""
                            print(f"\n[ROLLING] {label} ({completed_count}): {retry_resource.resource_name}{retry_info}")
                            print(f"          Status: {status}, Outcome: {outcome}, Duration: {duration:.1f}s")

                    except Exception as e:
                        print(f"\n[ROLLING] Exception ({completed_count}): {retry_resource.resource_name} - {e}")
                        all_results.append({
                            "resource_id": retry_resource.resource_id,
                            "resource_name": retry_resource.resource_name,
                            "status": "EXCEPTION",
                            "error": str(e),
                        })

                    # Remove from active futures
                    del active_futures[future]

                    # refill immediately after each completion
                    new_started = fill_available_slots()
                    if new_started > 0:
                        print(f"          Started {new_started} new pentest(s) to fill available slots")
                    # Process one completion at a time to keep refill snappy
                    break

            except FuturesTimeoutError:
                # No completions in the timeout period
                # Check if any retries are ready to start
                ready_retries = []
                remaining_retries = []

                while retry_queue:
                    retry_resource = retry_queue.popleft()
                    if retry_resource.can_retry():
                        ready_retries.append(retry_resource)
                    elif retry_resource.attempt <= MAX_START_RETRIES:
                        remaining_retries.append(retry_resource)

                # Put non-ready retries back
                retry_queue.extend(remaining_retries)

                # Try to start ready retries
                started_retries = 0
                for retry_resource in ready_retries:
                    if len(active_futures) < max_concurrent:
                        # Start this retry
                        retry_info = f" (retry {retry_resource.attempt}/{MAX_START_RETRIES})"
                        print(f"\n[ROLLING] Starting retry: {retry_resource.resource_name}{retry_info}")
                        if retry_resource.last_error:
                            print(f"          Previous error: {retry_resource.last_error}")

                        future = executor.submit(
                            run_pentest_for_resource,
                            retry_resource.resource_id,
                            retry_resource.resource_name,
                            template_id
                        )
                        active_futures[future] = (retry_resource, time.time())
                        started_retries += 1
                    else:
                        # No capacity, put back in queue
                        retry_queue.appendleft(retry_resource)
                        break

                if started_retries > 0:
                    print(f"          Started {started_retries} retry pentest(s)")

                continue

    print(f"\n[ROLLING] All pentests completed! Total: {len(all_results)} results")

    # Add any remaining retry queue items that exceeded max retries
    while retry_queue:
        retry_resource = retry_queue.popleft()
        print(f"[ROLLING] Resource {retry_resource.resource_name} exceeded max retries ({MAX_START_RETRIES})")
        all_results.append({
            "resource_id": retry_resource.resource_id,
            "resource_name": retry_resource.resource_name,
            "status": "START_FAILED_MAX_RETRIES",
            "error": retry_resource.last_error,
            "final_attempt": retry_resource.attempt,
        })

    return all_results


# ==============================
# Run Pentests with Rolling Parallelism and Retry
# ==============================

print(f"\n{'='*80}")
print(f"STARTING ROLLING PARALLEL PENTESTS FOR {len(SELECTED_RESOURCE_IDS)} RESOURCES")
print(f"Max concurrent: {MAX_CONCURRENT_PENTESTS} parallel tests")
print(f"{'='*80}")

all_results = run_rolling_parallel_with_retry(SELECTED_RESOURCE_IDS, resource_mapping, pentest_template_id, MAX_CONCURRENT_PENTESTS)

# ==============================
# 6. Final Results Summary and Exit Logic
# ==============================
print(f"\n{'='*80}")
print("FINAL ROLLING PARALLEL PENTEST RESULTS SUMMARY")
print(f"{'='*80}")
FAIL_ON_MODERATE = os.environ.get("FAIL_ON_MODERATE", "false").lower() == "true"

# Categorize results
completed_results = [r for r in all_results if r["status"] == "COMPLETED"]
timeout_continue_results = [r for r in all_results if r["status"] == "POLL_TIMEOUT_CONTINUE"]
timeout_partial_results = [r for r in all_results if r["status"] == "POLL_TIMEOUT_PARTIAL"]
extended_timeout_results = [r for r in all_results if r["status"] == "EXTENDED_POLL_TIMEOUT"]
start_failed_max_retries = [r for r in all_results if r["status"] == "START_FAILED_MAX_RETRIES"]
failed_results = [r for r in all_results if r["status"] not in [
    "COMPLETED", "POLL_TIMEOUT_CONTINUE", "POLL_TIMEOUT_PARTIAL",
    "EXTENDED_POLL_TIMEOUT", "START_FAILED_MAX_RETRIES"
]]

print(f"Total resources processed: {len(all_results)}")
print(f"Successfully completed: {len(completed_results)}")
print(f"Timed out (may still be running): {len(timeout_continue_results)}")
print(f"Timed out (partial results): {len(timeout_partial_results)}")
print(f"Extended timeout (still running): {len(extended_timeout_results)}")
print(f"Start failed (exceeded max retries): {len(start_failed_max_retries)}")
print(f"Failed or errored: {len(failed_results)}")

# Show failed resources
all_failed = failed_results + start_failed_max_retries
if all_failed:
    print("\nFailed Resources:")
    for result in all_failed:
        attempts_info = ""
        if result.get("final_attempt"):
            attempts_info = f" (after {result['final_attempt']} attempts)"
        print(f"  - {result['resource_name']}: {result['status']}{attempts_info}")

# Show timed out resources that may still be running
if timeout_continue_results:
    print("\nTimed Out Resources (may still be running on backend):")
    for result in timeout_continue_results:
        print(f"  - {result['resource_name']}: {result.get('message', 'Timed out')}")

# Show extended timeout resources (definitely still running)
if extended_timeout_results:
    print("\nExtended Timeout Resources (confirmed still running):")
    for result in extended_timeout_results:
        print(f"  - {result['resource_name']}: {result.get('message', 'Extended timeout')}")

# Show partial results
if timeout_partial_results:
    print("\nPartial Results (timed out but have outcome):")
    for result in timeout_partial_results:
        print(f"  - {result['resource_name']}: {result.get('outcome', 'Unknown')}")

# Analyze outcomes for successful pentests (include partial results)
outcomes: Dict[str, int] = {}
worst_outcome = "Excellent"

# Combine results that have outcomes
results_with_outcomes = completed_results + timeout_partial_results

if results_with_outcomes:
    print("\nOutcome Summary:")
    for result in results_with_outcomes:
        outcome = result.get("outcome", "Unknown")
        outcomes[outcome] = outcomes.get(outcome, 0) + 1
        status_indicator = "✓" if result["status"] == "COMPLETED" else "⚠"
        print(f"  {status_indicator} {result['resource_name']}: {outcome}")

        # Determine worst outcome for exit logic
        if outcome in ["Critical", "Poor"]:
            worst_outcome = "Critical"
        elif outcome == "Moderate" and worst_outcome not in ["Critical", "Poor"]:
            worst_outcome = "Moderate"
        elif outcome == "Good" and worst_outcome in ["Excellent"]:
            worst_outcome = "Good"

    print("\nOutcome Distribution:")
    for outcome, count in outcomes.items():
        print(f"  {outcome}: {count} resources")

# Exit based on worst outcome - be more lenient with timeouts
print(f"\nWorst outcome across all resources: {worst_outcome}")

# Count actual failures vs timeouts
actual_failures = len([r for r in all_results if r["status"] not in [
    "COMPLETED", "POLL_TIMEOUT_CONTINUE", "POLL_TIMEOUT_PARTIAL", "POLL_TIMEOUT", "EXTENDED_POLL_TIMEOUT"
]])
timeout_failures = len([r for r in all_results if r["status"] in ["POLL_TIMEOUT"]])

# Decide exit code, save JSON, then exit
exit_code = 0

if actual_failures > 0:
    print(f"❌ {actual_failures} pentests failed to start or had errors. Marking workflow as failed.")
    exit_code = 1
elif worst_outcome in ["Critical", "Poor"]:
    print("❌ At least one pentest outcome is severe. Marking workflow as failed.")
    exit_code = 1
elif worst_outcome == "Moderate":
    if FAIL_ON_MODERATE:
        print("❌ Pentest outcome is moderate and fail-on-moderate is enabled. Marking workflow as failed.")
        exit_code = 1
    else:
        print("⚠️ Worst pentest outcome is moderate. Marking workflow as neutral.")
        exit_code = 78
elif timeout_failures > 0 and len(results_with_outcomes) == 0:
    print(f"❌ All pentests timed out with no results. Marking workflow as failed.")
    exit_code = 1
elif timeout_failures > 0:
    print(f"⚠️ {timeout_failures} pentests timed out, but {len(results_with_outcomes)} have results. Marking workflow as neutral.")
    exit_code = 78
elif worst_outcome in ["Good", "Excellent"]:
    print("✅ All pentest outcomes are acceptable. Marking workflow as successful.")
    exit_code = 0
else:
    print(f"❓ Unexpected worst outcome: {worst_outcome}. Treating as failure.")
    exit_code = 1

# Save detailed results to JSON BEFORE exiting
results_filename = "pentest_results_summary.json"
with open(results_filename, "w") as f:
    json.dump(all_results, f, indent=2, default=str)
print(f"\n[+] Detailed results saved to {results_filename}")

exit(exit_code)
