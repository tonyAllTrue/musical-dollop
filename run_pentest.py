import os
import time
import json
import requests
import random

# ==============================
# Config
# ==============================
from dotenv import load_dotenv

# Load .env file only if running locally
if os.getenv("CI") != "true":  # GitHub Actions sets CI=true
    load_dotenv()


def get_env_var(key: str, required: bool = True) -> str:
    value = os.getenv(key)
    if required and not value:
        raise ValueError(f"Missing required environment variable: {key}")
    return value


API_URL = os.environ.get("API_URL")
API_KEY = os.environ.get("API_KEY")
CUSTOMER_ID = os.environ.get("CUSTOMER_ID")
ORGANIZATION_ID = os.environ.get("ORGANIZATION_ID")

if not all([API_URL, API_KEY, CUSTOMER_ID, ORGANIZATION_ID]):
    raise ValueError("Missing one or more required endpoint environment variables.")

# ==============================
# Auth
# ==============================


def get_jwt_token(api_key: str) -> str:
    endpoint = f"{API_URL}/v1/auth/issue-jwt-token"
    headers = {"X-API-Key": api_key, "Accept": "application/json"}
    resp = requests.post(endpoint, headers=headers)
    resp.raise_for_status()
    return resp.json()["access_token"]


JWT_TOKEN = get_jwt_token(API_KEY)
print("[+] JWT token obtained successfully.")


def _mask(s: str, show: int = 4) -> str:
    if not s:
        return s
    return s[:show] + "…" if len(s) > show else "…"


def make_api_request(
    endpoint: str,
    token: str,
    method: str = "GET",
    data=None,
    params=None,
    include_api_key: bool = False,
) -> requests.Response:
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "Accept": "application/json",
    }
    if include_api_key:
        headers["X-API-Key"] = API_KEY

    url = f"{API_URL}{endpoint}"
    resp = requests.request(method, url, headers=headers, params=params, json=data)
    try:
        resp.raise_for_status()
    except requests.HTTPError:
        # Print helpful but safe diagnostics
        safe_headers = {
            k: (v if k.lower() not in {"authorization", "x-api-key"} else _mask(v))
            for k, v in headers.items()
        }
        print(f"[-] {method} {url} failed: {resp.status_code}")
        print(f"    Headers sent: {safe_headers}")
        print(f"    Response: {resp.text}")
        raise
    return resp


# ==============================
# 1. Get Existing LLM Endpoints
# ==============================
print("\n[1] Fetching existing LLM endpoints...")
api = f"/v1/inventory/customer/{CUSTOMER_ID}/resources"
params = {
    "organization": ORGANIZATION_ID,
    "resource_category": "llm_endpoint",
    "omit_not_ai": "true",
}

resp = make_api_request(api, token=JWT_TOKEN, method="GET", params=params)
resources = resp.json()["resources"]

print(f"[+] Found {len(resources)} LLM endpoints")
# Filter for only resources with valid pentest connection details
valid_resources = [r for r in resources if r.get("has_valid_pentest_connection_details", False)]
print(f"[+] {len(valid_resources)} endpoints have valid pentest connection details")

# Create a mapping of resource_id to resource_name for easier tracking
resource_mapping = {}
for resource in valid_resources:
    resource_id = resource["resource_instance_id"]
    resource_name = resource["resource_display_name"]
    resource_mapping[resource_id] = resource_name
    print(f"    - {resource_name}")
    print(f"      ID: {resource_id}")
    print()

SELECTED_RESOURCE_IDS = list(resource_mapping.keys())
print(f"[+] Total selected resources: {len(SELECTED_RESOURCE_IDS)}")

# ==============================
# 2. Select Pentest Template
# ==============================
print("\n[2] Listing pentest templates...")
api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/templates"
resp = make_api_request(api, token=JWT_TOKEN, method="GET")
templates = resp.json()["llm_pentest_scan_templates"]

# Select template by name
TARGET_TEMPLATE_NAME = os.environ.get("TARGET_TEMPLATE_NAME", "CI/CD Pentest All Categories")
pentest_template_id = None

for template in templates:
    if template["name"] == TARGET_TEMPLATE_NAME:
        pentest_template_id = template["llm_pentest_scan_template_id"]
        print(f"[+] Found template '{TARGET_TEMPLATE_NAME}' with ID: {pentest_template_id}")
        break

print(f"[+] Using Pentest Template ID: {pentest_template_id}")

# ==============================
# Helper Functions for Steps 3-6
# ==============================


def run_pentest_for_resource(resource_id: str, resource_name: str, template_id: str) -> dict:
    """Run complete pentest workflow for a single resource"""
    print(f"\n{'='*60}")
    print(f"Starting pentest for: {resource_name}")
    print(f"Resource ID: {resource_id}")
    print(f"{'='*60}")

    # Step 3: Start Pentest
    print(f"\n[3] Starting pentest for {resource_name}...")
    api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/start-pentest"
    data = {
        "resource_instance_id": resource_id,
        "llm_pentest_scan_template_id": template_id,
        "description": f"CI/CD Pentest for {resource_name}",
        "pentest_connection_details": {},
    }

    try:
        resp = make_api_request(api, token=JWT_TOKEN, method="POST", data=data)
        resp_json = resp.json()

        job_id = resp_json["job_id"]
        scan_execution_id = resp_json["llm_pentest_scan_execution_id"]
        print(f"[+] Pentest Execution ID: {scan_execution_id}")
        print(f"[+] Pentest Job ID: {job_id}")
    except Exception as e:
        print(f"[-] Failed to start pentest for {resource_name}: {e}")
        return {
            "resource_id": resource_id,
            "resource_name": resource_name,
            "status": "START_FAILED",
            "error": str(e),
        }

    # Step 4: Poll for Pentest Results
    print(f"\n[4] Checking pentest execution status for {resource_name}...")
    job_status_api = f"/v2/llm-pentest/job-status/{job_id}"

    # --- Polling controls (tweak via env) ---
    POLL_INTERVAL_SECS = float(os.getenv("POLL_INTERVAL_SECS", "60"))  # normal interval between successful polls
    POLL_TIMEOUT_SECS = float(os.getenv("POLL_TIMEOUT_SECS", "3600"))  # overall timeout (default 60m)
    BACKOFF_BASE_SECS = float(os.getenv("POLL_BACKOFF_BASE_SECS", "5"))  # backoff base for failed polls
    BACKOFF_MAX_SECS = float(os.getenv("POLL_BACKOFF_MAX_SECS", "300"))  # cap backoff (default 5m)
    NOT_FOUND_GRACE_POLLS = int(os.getenv("POLL_NOT_FOUND_GRACE", "3"))  # brief grace while job propagates

    start_ts = time.time()
    status = None
    retry_streak = 0
    not_found_streak = 0

    while True:
        # Check overall timeout
        elapsed = time.time() - start_ts
        if elapsed >= POLL_TIMEOUT_SECS:
            print(f"[-] Polling timed out after {int(POLL_TIMEOUT_SECS)}s for {resource_name}.")
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "POLL_TIMEOUT",
                "error": f"Timed out after {int(POLL_TIMEOUT_SECS)} seconds",
            }

        try:
            resp = make_api_request(job_status_api, token=JWT_TOKEN, method="GET", include_api_key=True)
            resp_json = resp.json()
            status = resp_json.get("status")
            print(f"    Current status for {resource_name}: {status}")

            # Any successful call resets backoff
            retry_streak = 0

            if status == "COMPLETED":
                break

            if status == "FAILED":
                print(f"[-] Pentest failed for {resource_name}")
                return {"resource_id": resource_id, "resource_name": resource_name, "status": "PENTEST_FAILED"}

            if status == "NOT_FOUND":
                # Give a short grace period for eventual consistency
                not_found_streak += 1
                if not_found_streak <= NOT_FOUND_GRACE_POLLS:
                    time.sleep(min(POLL_INTERVAL_SECS, 10))
                    continue
                # After grace, keep polling until overall timeout (treat as transient)
                print(f"[!] job-status returned NOT_FOUND (streak={not_found_streak}); continuing to poll…")
            else:
                # Reset NOT_FOUND streak once we see any other status
                not_found_streak = 0

            # Still running / unknown -> wait normal interval
            time.sleep(POLL_INTERVAL_SECS)

        except requests.RequestException as e:
            # Treat all request errors as transient during the polling window
            retry_streak += 1
            code = getattr(getattr(e, "response", None), "status_code", None)

            # Exponential backoff with jitter
            raw_delay = BACKOFF_BASE_SECS * (2 ** min(retry_streak, 6))  # cap exponent growth
            delay = min(BACKOFF_MAX_SECS, raw_delay) * (0.8 + 0.4 * random.random())

            print(
                f"[!] Temporary polling error for {resource_name} (attempt {retry_streak}, status={code}). "
                f"Retrying in {delay:.1f}s. Error: {e}"
            )
            time.sleep(delay)
            # loop continues

    # If we get here, status == COMPLETED
    print(f"[+] Pentest completed for {resource_name}. Downloading results...")
    download_api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/executions/{scan_execution_id}/download-csv"
    try:
        resp = requests.post(
            f"{API_URL}{download_api}",
            headers={
                "Authorization": f"Bearer {JWT_TOKEN}",
                "Content-Type": "application/json",
            },
        )
        if resp.status_code == 200:
            # Create safe filename
            safe_name = "".join(c for c in resource_name if c.isalnum() or c in (" ", "-", "_")).rstrip()
            filename = f"pentest_results_{safe_name}_{resource_id[:8]}.csv"
            with open(filename, "wb") as f:
                f.write(resp.content)
            print(f"[+] CSV saved as {filename}")
        else:
            print(f"[-] Download failed for {resource_name}: {resp.status_code} - {resp.text}")
    except Exception as e:
        print(f"[-] Error downloading results for {resource_name}: {e}")

    # Step 5: GraphQL Outcome Evaluation
    print(f"\n[5] Evaluating pentest outcome for {resource_name}...")

    graphql_endpoint = f"{API_URL}/v2/graphql"
    graphql_query = """
    query PentestExecution($customerId: UUID!, $execId: UUID!) {
      llmPentestScanExecution(
        filter: {
          customerId: $customerId,
          llmPentestScanExecutionId: $execId
        }
      ) {
        taskId
        finishedAt
        description
        outcomeLevel
        chosenLlmModel
        reportGenerationJobId
      }
    }
    """

    variables = {
        "customerId": CUSTOMER_ID,
        "execId": scan_execution_id,
    }

    headers = {
        "Authorization": f"Bearer {JWT_TOKEN}",
        "Content-Type": "application/json",
        "Accept": "application/json",
    }

    try:
        resp = requests.post(
            graphql_endpoint,
            headers=headers,
            json={"query": graphql_query, "variables": variables},
        )
        resp.raise_for_status()

        gql = resp.json()
        if "errors" in gql:
            print(f"[-] GraphQL returned errors for {resource_name}:")
            print(json.dumps(gql["errors"], indent=2))
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "GRAPHQL_ERROR",
                "errors": gql["errors"],
            }

        execution = gql.get("data", {}).get("llmPentestScanExecution")
        if not execution:
            print(f"[-] No llmPentestScanExecution in GraphQL response for {resource_name}")
            return {
                "resource_id": resource_id,
                "resource_name": resource_name,
                "status": "NO_EXECUTION_DATA",
            }

        outcome = (execution.get("outcomeLevel") or "").strip()
        print(f"[+] Outcome Level for {resource_name}: {outcome}")

        return {
            "resource_id": resource_id,
            "resource_name": resource_name,
            "status": "COMPLETED",
            "outcome": outcome,
            "execution_details": execution,
            "scan_execution_id": scan_execution_id,
            "job_id": job_id,
        }

    except Exception as e:
        print(f"[-] Error evaluating outcome for {resource_name}: {e}")
        return {
            "resource_id": resource_id,
            "resource_name": resource_name,
            "status": "EVALUATION_FAILED",
            "error": str(e),
        }


# ==============================
# Run Pentests for All Resources
# ==============================
print(f"\n{'='*60}")
print(f"STARTING PENTESTS FOR {len(SELECTED_RESOURCE_IDS)} RESOURCES")
print(f"{'='*60}")

all_results = []
for resource_id in SELECTED_RESOURCE_IDS:
    resource_name = resource_mapping[resource_id]
    result = run_pentest_for_resource(resource_id, resource_name, pentest_template_id)
    all_results.append(result)

    # Brief pause between resources to avoid overwhelming the API
    if len(SELECTED_RESOURCE_IDS) > 1:
        print("\nWaiting 30 seconds before starting next resource...")
        time.sleep(30)

# ==============================
# 6. Final Results Summary and Exit Logic
# ==============================
print(f"\n{'='*60}")
print("FINAL PENTEST RESULTS SUMMARY")
print(f"{'='*60}")
FAIL_ON_MODERATE = os.environ.get("FAIL_ON_MODERATE", "false").lower() == "true"

# Categorize results
completed_results = [r for r in all_results if r["status"] == "COMPLETED"]
failed_results = [r for r in all_results if r["status"] != "COMPLETED"]

print(f"Total resources processed: {len(all_results)}")
print(f"Successfully completed: {len(completed_results)}")
print(f"Failed or errored: {len(failed_results)}")

# Show failed resources
if failed_results:
    print("\nFailed Resources:")
    for result in failed_results:
        print(f"  - {result['resource_name']}: {result['status']}")

# Analyze outcomes for successful pentests
outcomes = {}
worst_outcome = "Excellent"  # Start with best possible outcome

if completed_results:
    print("\nOutcome Summary:")
    for result in completed_results:
        outcome = result.get("outcome", "Unknown")
        outcomes[outcome] = outcomes.get(outcome, 0) + 1
        print(f"  - {result['resource_name']}: {outcome}")

        # Determine worst outcome for exit logic
        if outcome in ["Critical", "Poor"]:
            worst_outcome = "Critical"
        elif outcome == "Moderate" and worst_outcome not in ["Critical", "Poor"]:
            worst_outcome = "Moderate"
        elif outcome == "Good" and worst_outcome in ["Excellent"]:
            worst_outcome = "Good"

    print("\nOutcome Distribution:")
    for outcome, count in outcomes.items():
        print(f"  {outcome}: {count} resources")

# Save detailed results to JSON
results_filename = "pentest_results_summary.json"
with open(results_filename, "w") as f:
    json.dump(all_results, f, indent=2, default=str)
print(f"\n[+] Detailed results saved to {results_filename}")

# Exit based on worst outcome
print(f"\nWorst outcome across all resources: {worst_outcome}")

if failed_results:
    print("❌ Some pentests failed to complete. Marking workflow as failed.")
    exit(1)
elif worst_outcome in ["Critical", "Poor"]:
    print("❌ At least one pentest outcome is severe. Marking workflow as failed.")
    exit(1)
elif worst_outcome == "Moderate":
    if FAIL_ON_MODERATE:
        print("❌ Pentest outcome is moderate and fail-on-moderate is enabled. Marking workflow as failed.")
        exit(1)
    else:
        print("⚠️ Worst pentest outcome is moderate. Marking workflow as neutral.")
        exit(78)
elif worst_outcome in ["Good", "Excellent"]:
    print("✅ All pentest outcomes are acceptable. Marking workflow as successful.")
    exit(0)
else:
    print(f"❓ Unexpected worst outcome: {worst_outcome}. Treating as failure.")
    exit(1)

