from __future__ import annotations

import sys

import config
import auth
import api
from pentest import run_rolling_parallel_with_retry
from summary import finalize_and_exit, finalize_model_scan
from llm_endpoints import select_llm_endpoints
from model_scan import select_models_and_assets, run_model_scans


def main() -> int:
    config.print_config_banner()

    jwt = auth.get_jwt_token(config.API_KEY)
    print("[+] JWT token obtained successfully.")

    overall_exit = 0

    # ---------------- LLM Pentest phase (optional) ----------------
    if config.ENABLE_LLM_PENTEST:
        selected_ids, resource_mapping = select_llm_endpoints(jwt)
        if not selected_ids:
            print("✖ No LLM endpoint resources selected. (Pentest phase)")
        else:
            print("\n[2] Listing pentest templates…")
            templates = api.list_pentest_templates(jwt)
            target_name = config.TARGET_TEMPLATE_NAME
            template_id = next(
                (t["llm_pentest_scan_template_id"] for t in templates if t.get("name") == target_name),
                None,
            )

            if template_id is None:
                print(f"✖ Pentest template '{target_name}' was not found.")
                available = ", ".join(t.get("name", "<unnamed>") for t in templates) or "(no templates returned)"
                print(f"    Available templates: {available}")
                print("    Tip: Set TARGET_TEMPLATE_NAME env var to one of the available templates, or create the template in the UI/API.")
            else:
                print(f"[+] Found template '{target_name}' with ID: {template_id}")
                print(f"[+] Using Pentest Template ID: {template_id}")

                print(f"\n{'='*80}")
                print(f"STARTING ROLLING PARALLEL PENTESTS FOR {len(selected_ids)} RESOURCES")
                print(f"Max concurrent: {config.MAX_CONCURRENT_PENTESTS} parallel tests")
                print(f"{'='*80}")

                all_results = run_rolling_parallel_with_retry(
                    selected_ids,
                    resource_mapping,
                    template_id,
                    config.MAX_CONCURRENT_PENTESTS,
                )

                # finalize pentest phase (threshold / GH issues handled inside)
                pentest_exit = finalize_and_exit(all_results)
                overall_exit = max(overall_exit, pentest_exit)
    else:
        print("ℹ️ ENABLE_LLM_PENTEST is false; skipping LLM pentest phase.")

    # ---------------- Model scanning phase (optional) ----------------
    if config.ENABLE_MODEL_SCANNING:
        print("\n[3] Selecting model/model_asset resources for scanning…")
        ms_ids, ms_map = select_models_and_assets(jwt)
        if not ms_ids:
            print("✖ No model/model_asset resources selected. (Model scanning phase)")
        else:
            ms_results = run_model_scans(jwt, ms_ids, ms_map)

            model_exit = finalize_model_scan(ms_results)  # prints summary + writes model_scan_results_summary.json
            overall_exit = max(overall_exit, model_exit)
    else:
        print("ℹ️ ENABLE_MODEL_SCANNING is false; skipping model scanning phase.")

    return overall_exit


if __name__ == "__main__":
    sys.exit(main())