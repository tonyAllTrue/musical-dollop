import sys
from typing import Dict, List

import config
import auth
import api
from pentest import run_rolling_parallel_with_retry
from summary import finalize_and_exit, finalize_model_scan

from model_scan import (
    select_models_and_assets,  
    run_model_scans,           
)

from github_issues import create_issues_for_model_scan_violations


def select_resources(jwt: str) -> tuple[List[str], Dict[str, str]]:
    """Return (selected_ids, resource_mapping) based on INVENTORY_SCOPE and env filters for LLM endpoints."""
    scope = config.INVENTORY_SCOPE
    print(f"\n[0] Inventory selection scope: {scope}")

    all_found: list[dict] = []

    if scope == "organization":
        print("[0] Fetching LLM endpoints for organization scope‚Ä¶")
        if config.ORGANIZATION_ID:
            print(f"[0] Using organization filter: {config.ORGANIZATION_ID}")
        else:
            print("[i] No ORGANIZATION_ID set; fetching across all orgs for this customer.")
        all_found = api.list_llm_endpoints(
            jwt_token=jwt,
            organization_id=config.ORGANIZATION_ID or None,
            project_id=None,
            valid_only=config.HAS_VALID_PENTEST_CONNECTION_DETAILS,
        )

    elif scope == "project":
        if not config.PROJECT_IDS:
            print("‚úñ INVENTORY_SCOPE=project but PROJECT_IDS is empty.")
            return [], {}
        print(f"[0] Fetching LLM endpoints for projects: {config.PROJECT_IDS}")
        batch: list[dict] = []
        for pid in config.PROJECT_IDS:
            res = api.list_llm_endpoints(
                jwt_token=jwt,
                organization_id=None,
                project_id=pid,
                valid_only=config.HAS_VALID_PENTEST_CONNECTION_DETAILS,
            )
            print(f"    - Project {pid}: {len(res)} endpoints")
            batch.extend(res)
        all_found = api.dedupe_resources(batch)

    elif scope == "resource":
        print("[0] Fetching LLM endpoints for resource scope (will filter by IDs/names)‚Ä¶")
        candidates = api.list_llm_endpoints(
            jwt_token=jwt,
            organization_id=None,
            project_id=None,
            valid_only=config.HAS_VALID_PENTEST_CONNECTION_DETAILS,
        )

        by_id = set(r.lower() for r in config.TARGET_RESOURCE_IDS)
        by_name = [s.lower() for s in config.TARGET_RESOURCE_NAMES]

        if not by_id and not by_name:
            print("‚úñ INVENTORY_SCOPE=resource but no TARGET_RESOURCE_IDS or TARGET_RESOURCE_NAMES provided.")
            return [], {}

        def name_match(display: str) -> bool:
            dl = (display or "").lower()
            return any(substr in dl for substr in by_name)

        filtered: list[dict] = []
        for r in candidates:
            rid = (r.get("resource_instance_id") or "").lower()
            rname = r.get("resource_display_name") or ""
            if (by_id and rid in by_id) or (by_name and name_match(rname)):
                filtered.append(r)

        print(f"[0] Resource scope: matched {len(filtered)} of {len(candidates)} candidates")
        all_found = api.dedupe_resources(filtered)

    else:
        print(f"‚úñ Unknown INVENTORY_SCOPE='{scope}'. Use organization|project|resource.")
        return [], {}

    print(f"[+] Found {len(all_found)} LLM endpoints (post-scope)")
    if config.HAS_VALID_PENTEST_CONNECTION_DETAILS:
        valid = [r for r in all_found if r.get("has_valid_pentest_connection_details", False)]
        print(f"[+] {len(valid)} endpoints have valid pentest connection details")
        chosen = valid
    else:
        chosen = all_found
        valid_ct = sum(1 for r in all_found if r.get("has_valid_pentest_connection_details", False))
        print(f"[i] Valid-pentest-connection filter disabled; of these, {valid_ct} have valid details")

    resource_mapping: Dict[str, str] = {}
    for resource in chosen:
        rid = resource["resource_instance_id"]
        rname = resource["resource_display_name"]
        resource_mapping[rid] = rname
        print(f"    - {rname}\n      ID: {rid}\n")

    selected_ids = list(resource_mapping.keys())
    print(f"[+] Total selected resources: {len(selected_ids)}")
    return selected_ids, resource_mapping


def main() -> int:
    config.print_config_banner()

    jwt = auth.get_jwt_token(config.API_KEY)
    print("[+] JWT token obtained successfully.")

    overall_exit = 0

    # ---------------- LLM Pentest phase (optional) ----------------
    if config.ENABLE_LLM_PENTEST:
        selected_ids, resource_mapping = select_resources(jwt)
        if not selected_ids:
            print("‚úñ No LLM endpoint resources selected. (Pentest phase)")
        else:
            print("\n[2] Listing pentest templates‚Ä¶")
            templates = api.list_pentest_templates(jwt)
            target_name = config.TARGET_TEMPLATE_NAME
            template_id = next((t["llm_pentest_scan_template_id"] 
                              for t in templates 
                              if t.get("name") == target_name), None)

            if template_id is None:
                print(f"‚úñ Pentest template '{target_name}' was not found.")
                available = ", ".join(t.get("name", "<unnamed>") for t in templates) or "(no templates returned)"
                print(f"    Available templates: {available}")
                print("    Tip: Set TARGET_TEMPLATE_NAME env var to one of the available templates, or create the template in the UI/API.")
            else:
                print(f"[+] Found template '{target_name}' with ID: {template_id}")
                print(f"[+] Using Pentest Template ID: {template_id}")

                print(f"\n{'='*80}")
                print(f"STARTING ROLLING PARALLEL PENTESTS FOR {len(selected_ids)} RESOURCES")
                print(f"Max concurrent: {config.MAX_CONCURRENT_PENTESTS} parallel tests")
                print(f"{'='*80}")

                all_results = run_rolling_parallel_with_retry(
                    selected_ids,
                    resource_mapping,
                    template_id,
                    config.MAX_CONCURRENT_PENTESTS
                )

                # finalize pentest phase (threshold / GH issues handled inside)
                pentest_exit = finalize_and_exit(all_results)
                overall_exit = max(overall_exit, pentest_exit)
    else:
        print("‚ÑπÔ∏è ENABLE_LLM_PENTEST is false; skipping LLM pentest phase.")

    # ---------------- Model scanning phase (optional) ----------------
    if config.ENABLE_MODEL_SCANNING:
        print("\n[3] Selecting model/model_asset resources for scanning‚Ä¶")
        ms_ids, ms_map = select_models_and_assets(jwt)
        if not ms_ids:
            print("‚úñ No model/model_asset resources selected. (Model scanning phase)")
        else:
            ms_results = run_model_scans(jwt, ms_ids, ms_map)

            # open GH issues for any model-scan violations
            created_ms = create_issues_for_model_scan_violations(ms_results)
            if created_ms:
                print(f"üé´ Created {created_ms} GitHub issue(s) for model-scan violations.")

            model_exit = finalize_model_scan(ms_results)  # prints summary + writes model_scan_results_summary.json
            overall_exit = max(overall_exit, model_exit)
    else:
        print("‚ÑπÔ∏è ENABLE_MODEL_SCANNING is false; skipping model scanning phase.")

    return overall_exit


if __name__ == "__main__":
    sys.exit(main())