import os
import time
import json
import requests

# ==============================
# Config
# ==============================
from dotenv import load_dotenv
# Load .env file only if running locally
if os.getenv("CI") != "true":  # GitHub Actions sets CI=true
    load_dotenv()

def get_env_var(key: str, required: bool = True) -> str:
    value = os.getenv(key)
    if required and not value:
        raise ValueError(f"Missing required environment variable: {key}")
    return value

API_URL = os.environ.get("API_URL")
API_KEY = os.environ.get("API_KEY")
CUSTOMER_ID = os.environ.get("CUSTOMER_ID")
ORGANIZATION_ID = os.environ.get("ORGANIZATION_ID")

if not all([API_URL, API_KEY, CUSTOMER_ID, ORGANIZATION_ID]):
    raise ValueError("Missing one or more required endpoint environment variables.")

# ==============================
# Auth
# ==============================
def get_jwt_token(api_key: str) -> str:
    endpoint = f"{API_URL}/v1/auth/issue-jwt-token"
    headers = {"X-API-Key": api_key, "Accept": "application/json"}
    resp = requests.post(endpoint, headers=headers)
    resp.raise_for_status()
    return resp.json()["access_token"]

JWT_TOKEN = get_jwt_token(API_KEY)
print("[+] JWT token obtained successfully.")

def _mask(s: str, show: int = 4) -> str:
    if not s:
        return s
    return s[:show] + "…" if len(s) > show else "…"

def make_api_request(endpoint: str, token: str, method: str = "GET", data=None, params=None, include_api_key: bool = False) -> requests.Response:
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "Accept": "application/json",
    }
    if include_api_key:
        headers["X-API-Key"] = API_KEY

    url = f"{API_URL}{endpoint}"
    resp = requests.request(method, url, headers=headers, params=params, json=data)
    try:
        resp.raise_for_status()
    except requests.HTTPError:
        # Print helpful but safe diagnostics
        safe_headers = {k: (v if k.lower() not in {"authorization", "x-api-key"} else _mask(v)) for k, v in headers.items()}
        print(f"[-] {method} {url} failed: {resp.status_code}")
        print(f"    Headers sent: {safe_headers}")
        print(f"    Response: {resp.text}")
        raise
    return resp

# ==============================
# 1. Get Existing LLM Endpoints
# ==============================
print("\n[1] Fetching existing LLM endpoints...")
api = f"/v1/inventory/customer/{CUSTOMER_ID}/resources"
params = {
    "organization": ORGANIZATION_ID,
    "resource_category": "llm_endpoint",
    "omit_not_ai": "true"
}

resp = make_api_request(api, token=JWT_TOKEN, method="GET", params=params)
resources = resp.json()["resources"]

print(f"[+] Found {len(resources)} LLM endpoints")
# Filter for only resources with valid pentest connection details
valid_resources = [r for r in resources if r.get("has_valid_pentest_connection_details", False)]
print(f"[+] {len(valid_resources)} endpoints have valid pentest connection details")

# Create a mapping of resource_id to resource_name for easier tracking
resource_mapping = {}
for resource in valid_resources:
    resource_id = resource["resource_instance_id"]
    resource_name = resource["resource_display_name"]
    resource_mapping[resource_id] = resource_name
    print(f"    - {resource_name}")
    print(f"      ID: {resource_id}")
    print()

SELECTED_RESOURCE_IDS = list(resource_mapping.keys())
print(f"[+] Total selected resources: {len(SELECTED_RESOURCE_IDS)}")

# ==============================
# 2. Select Pentest Template
# ==============================
print("\n[2] Listing pentest templates...")
api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/templates"
resp = make_api_request(api, token=JWT_TOKEN, method="GET")
templates = resp.json()["llm_pentest_scan_templates"]

# Select template by name
TARGET_TEMPLATE_NAME = os.environ.get("TARGET_TEMPLATE_NAME", "CI/CD Pentest All Categories")
pentest_template_id = None

for template in templates:
    if template["name"] == TARGET_TEMPLATE_NAME:
        pentest_template_id = template["llm_pentest_scan_template_id"]
        print(f"[+] Found template '{TARGET_TEMPLATE_NAME}' with ID: {pentest_template_id}")
        break

print(f"[+] Using Pentest Template ID: {pentest_template_id}")

# ==============================
# Helper Functions for Steps 3-6
# ==============================
def run_pentest_for_resource(resource_id: str, resource_name: str, template_id: str) -> dict:
    """Run complete pentest workflow for a single resource"""
    print(f"\n{'='*60}")
    print(f"Starting pentest for: {resource_name}")
    print(f"Resource ID: {resource_id}")
    print(f"{'='*60}")

    # Step 3: Start Pentest
    print(f"\n[3] Starting pentest for {resource_name}...")
    api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/start-pentest"
    data = {
        "resource_instance_id": resource_id,
        "llm_pentest_scan_template_id": template_id,
        "description": f"CI/CD Pentest for {resource_name}",
        "pentest_connection_details": {}
    }

    try:
        resp = make_api_request(api, token=JWT_TOKEN, method="POST", data=data)
        resp_json = resp.json()
        
        job_id = resp_json["job_id"]
        scan_execution_id = resp_json["llm_pentest_scan_execution_id"]
        print(f"[+] Pentest Execution ID: {scan_execution_id}")
        print(f"[+] Pentest Job ID: {job_id}")
    except Exception as e:
        print(f"[-] Failed to start pentest for {resource_name}: {e}")
        return {"resource_id": resource_id, "resource_name": resource_name, "status": "START_FAILED", "error": str(e)}
    
    # Step 4: Poll for Pentest Results
    print(f"\n[4] Checking pentest execution status for {resource_name}...")
    job_status_api = f"/v2/llm-pentest/job-status/{job_id}"
    
    while True:
        try:
            resp = make_api_request(job_status_api, token=JWT_TOKEN, method="GET", include_api_key=True)
            resp_json = resp.json()
            status = resp_json["status"]
            print(f"    Current status for {resource_name}: {status}")
            
            if status in ["COMPLETED", "FAILED", "NOT_FOUND"]:
                break
            
            time.sleep(60)
        except Exception as e:
            print(f"[-] Error polling status for {resource_name}: {e}")
            return {"resource_id": resource_id, "resource_name": resource_name, "status": "POLL_FAILED", "error": str(e)}
    
    if status == "COMPLETED":
        print(f"[+] Pentest completed for {resource_name}. Downloading results...")
        download_api = f"/v2/llm-pentest/customer/{CUSTOMER_ID}/executions/{scan_execution_id}/download-csv"
        try:
            resp = requests.post(f"{API_URL}{download_api}", headers={
                "Authorization": f"Bearer {JWT_TOKEN}",
                "Content-Type": "application/json"
            })
            
            if resp.status_code == 200:
                # Create safe filename
                safe_name = "".join(c for c in resource_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
                filename = f"pentest_results_{safe_name}_{resource_id[:8]}.csv"
                with open(filename, "wb") as f:
                    f.write(resp.content)
                print(f"[+] CSV saved as {filename}")
            else:
                print(f"[-] Download failed for {resource_name}: {resp.status_code} - {resp.text}")
        except Exception as e:
            print(f"[-] Error downloading results for {resource_name}: {e}")
    elif status == "FAILED":
        print(f"[-] Pentest failed for {resource_name}")
        return {"resource_id": resource_id, "resource_name": resource_name, "status": "PENTEST_FAILED"}
    
    # Step 5: GraphQL Outcome Evaluation
    print(f"\n[5] Evaluating pentest outcome for {resource_name}...")
    
    graphql_endpoint = f"{API_URL}/v2/graphql"
    graphql_query = """
    query PentestExecution($customerId: UUID!, $execId: UUID!) {
      llmPentestScanExecution(
        filter: {
          customerId: $customerId,
          llmPentestScanExecutionId: $execId
        }
      ) {
        taskId
        finishedAt
        description
        outcomeLevel
        chosenLlmModel
        reportGenerationJobId
      }
    }
    """
    
    variables = {
        "customerId": CUSTOMER_ID,
        "execId": scan_execution_id,
    }
    
    headers = {
        "Authorization": f"Bearer {JWT_TOKEN}",
        "Content-Type": "application/json",
        "Accept": "application/json",
    }
    
    try:
        resp = requests.post(graphql_endpoint, headers=headers, json={"query": graphql_query, "variables": variables})
        resp.raise_for_status()
        
        gql = resp.json()
        if "errors" in gql:
            print(f"[-] GraphQL returned errors for {resource_name}:")
            print(json.dumps(gql["errors"], indent=2))
            return {"resource_id": resource_id, "resource_name": resource_name, "status": "GRAPHQL_ERROR", "errors": gql["errors"]}
        
        execution = gql.get("data", {}).get("llmPentestScanExecution")
        if not execution:
            print(f"[-] No llmPentestScanExecution in GraphQL response for {resource_name}")
            return {"resource_id": resource_id, "resource_name": resource_name, "status": "NO_EXECUTION_DATA"}
        
        outcome = (execution.get("outcomeLevel") or "").strip()
        print(f"[+] Outcome Level for {resource_name}: {outcome}")
        
        return {
            "resource_id": resource_id,
            "resource_name": resource_name,
            "status": "COMPLETED",
            "outcome": outcome,
            "execution_details": execution,
            "scan_execution_id": scan_execution_id,
            "job_id": job_id
        }
        
    except Exception as e:
        print(f"[-] Error evaluating outcome for {resource_name}: {e}")
        return {"resource_id": resource_id, "resource_name": resource_name, "status": "EVALUATION_FAILED", "error": str(e)}
    
# ==============================
# Run Pentests for All Resources
# ==============================
print(f"\n{'='*60}")
print(f"STARTING PENTESTS FOR {len(SELECTED_RESOURCE_IDS)} RESOURCES")
print(f"{'='*60}")

all_results = []
for resource_id in SELECTED_RESOURCE_IDS:
    resource_name = resource_mapping[resource_id]
    result = run_pentest_for_resource(resource_id, resource_name, pentest_template_id)
    all_results.append(result)
    
    # Brief pause between resources to avoid overwhelming the API
    if len(SELECTED_RESOURCE_IDS) > 1:
        print(f"\nWaiting 30 seconds before starting next resource...")
        time.sleep(30)

# ==============================
# 6. Final Results Summary and Exit Logic
# ==============================
print(f"\n{'='*60}")
print("FINAL PENTEST RESULTS SUMMARY")
print(f"{'='*60}")
FAIL_ON_MODERATE = os.environ.get("FAIL_ON_MODERATE", "false").lower() == "true"

# Categorize results
completed_results = [r for r in all_results if r["status"] == "COMPLETED"]
failed_results = [r for r in all_results if r["status"] != "COMPLETED"]

print(f"Total resources processed: {len(all_results)}")
print(f"Successfully completed: {len(completed_results)}")
print(f"Failed or errored: {len(failed_results)}")

# Show failed resources
if failed_results:
    print(f"\nFailed Resources:")
    for result in failed_results:
        print(f"  - {result['resource_name']}: {result['status']}")

# Analyze outcomes for successful pentests
outcomes = {}
worst_outcome = "Excellent"  # Start with best possible outcome

if completed_results:
    print(f"\nOutcome Summary:")
    for result in completed_results:
        outcome = result.get("outcome", "Unknown")
        outcomes[outcome] = outcomes.get(outcome, 0) + 1
        print(f"  - {result['resource_name']}: {outcome}")
        
        # Determine worst outcome for exit logic
        if outcome in ["Critical", "Poor"]:
            worst_outcome = "Critical"
        elif outcome == "Moderate" and worst_outcome not in ["Critical", "Poor"]:
            worst_outcome = "Moderate"
        elif outcome == "Good" and worst_outcome in ["Excellent"]:
            worst_outcome = "Good"

    print(f"\nOutcome Distribution:")
    for outcome, count in outcomes.items():
        print(f"  {outcome}: {count} resources")

# Save detailed results to JSON
results_filename = "pentest_results_summary.json"
with open(results_filename, "w") as f:
    json.dump(all_results, f, indent=2, default=str)
print(f"\n[+] Detailed results saved to {results_filename}")

# Exit based on worst outcome
print(f"\nWorst outcome across all resources: {worst_outcome}")

if failed_results:
    print("❌ Some pentests failed to complete. Marking workflow as failed.")
    exit(1)
elif worst_outcome in ["Critical", "Poor"]:
    print("❌ At least one pentest outcome is severe. Marking workflow as failed.")
    exit(1)
elif worst_outcome == "Moderate":
    if FAIL_ON_MODERATE:
        print("❌ Pentest outcome is moderate and fail-on-moderate is enabled. Marking workflow as failed.")
        exit(1)
    else:
        print("⚠️ Worst pentest outcome is moderate. Marking workflow as neutral.")
        exit(78)
elif worst_outcome in ["Good", "Excellent"]:
    print("✅ All pentest outcomes are acceptable. Marking workflow as successful.")
    exit(0)
else:
    print(f"❓ Unexpected worst outcome: {worst_outcome}. Treating as failure.")
    exit(1)
